<h2>Welcome to Strategic Data Skills</h2>
<section>
<div>
<h4>Strategic Data Skills</h4>
<p><strong>Welcome to the Strategic Data Skills.</strong></p>
Please watch the video below for an Introduction to Strategic Data Skills<a href="https://player.vimeo.com/video/841919886?h=17bc091133" target="_blank" rel="noopener">video link</a>
<div>
<h4>Course overview</h4>
<p><strong>The course aims to develop greater skills and knowledge in:</strong></p>
<ul>
<ul>
<li>​​Identifying how open and shared data infrastructures can improve decision making</li>
<li>Applying strategic thinking to new data projects</li>
<li>Open and shared approaches that make it easier to create and build impactful products and services</li>
<li>Making better decisions with data</li>
</ul>
</ul>
This course is made up of 5 modules, each module should take between 2 and 3 hours to complete.Below is a list of the upcoming topics we will cover:
<ol>
<li>Building a healthy data ecosystem&nbsp;</li>
<li>Organising and validating data&nbsp;</li>
<li>Introduction to data analysis</li>
<li>Telling stories with data</li>
<li>Data ethics</li>
</ol>
</div>
</div>
<div>
<h3>Obtaining your Certificate of Completion</h3>
<p>In order to get the most out of the course, we encourage learners to engage with the content in all modules. The course has been built sequentially, and builds upon the experiences of each module.</p>
<p>To obtain your certificate, learners must complete the summative activities. There is not a minimum grade to achieve, but the following activities must be attempted:</p>
<ol>
<li>Module 2: Hands-on: organising and structuring data</li>
<li>Module 3: Analysing the performance of the London Fire Brigade</li>
<li>Module 4: Telling the story of the London Fire Brigade</li>
</ol>
<p>There is no limit on how many times you can undertake an activity. You will receive feedback on each activity, either through pre-written outcomes or via the Tutor's AI assistant who will provide personalised feedback.</p>
<p>Once the completion conditions are met, the Course Completion topic will be shown and provide information for learners to access their completion certificate.</p>
</div>
</section>
<h2>About the ODI</h2>
<section>
<div>
<p>The ODI is a non-profit company, founded in 2012 by Sir Tim Berners-Lee and Sir Nigel Shadbolt. Since 2017, the ODI&rsquo;s operating costs of around &pound;6m per annum have been met through a range of grants and commercial revenue.</p>
<p>Many of our products and services are available openly and for free, including our&nbsp;reports,&nbsp;tools&nbsp;and&nbsp;webinars.</p>
</div>
<div>
<h3>What we believe</h3>
<p>At the ODI, our mission is&nbsp;<strong>to work with companies and governments to build an open, trustworthy data ecosystem.</strong></p>
<p>We want a world where data works for everyone. This means getting data to those who need it, particularly in response to the&nbsp;UN Sustainable Development Goals.</p>
<p>To achieve our mission, we:</p>
<ul>
<li>work with organisations to help them use better data practices</li>
<li>work with sectors and regions to ensure data is helping them</li>
<li>help interpret and apply the latest thinking around data and&nbsp;data infrastructure.</li>
</ul>
</div>
<div>
<h3>What are we?</h3>
<p>The ODI operates as an institute, and we also undertake commercial work that advances our mission. We:</p>
<ul>
<li>work in policy &ndash; influencing government decisions towards the kind of data ecosystem we want to create that will be best for society</li>
<li>work with businesses &ndash; helping them to develop ethical, equitable data infrastructure that will be both good for business and their social mission</li>
<li>work with philanthropic and other grant-giving organisations &ndash; to build a data infrastructure that brings benefits to all</li>
<li>work with governments, research organisations, public bodies and civil society around the world &ndash; to ensure that they can benefit from better data infrastructure.</li>
</ul>
<p>Our work includes a range of activities including conducting&nbsp;applied research, delivering consultancy services and training, and providing free tools and courses. We have an active and engaged membership community, over 17,000 weekly newsletter subscribers, and hosted more than 1,000 attendees at our recent summit. Since 2012, we have told data stories through our&nbsp;Data as Culture arts programme&nbsp;which includes over 100 artworks.</p>
</div>
<div>
<h3>The work we do</h3>
<p>The ODI aims to enable the development of&nbsp;<em>data infrastructure&nbsp;</em>in ways that benefit people, companies, governments and civil society. We focus on increasing&nbsp;<strong>data flows around the data ecosystem</strong>, improving skills and capabilities, and encouraging innovation. We support data flows, focusing&nbsp;our efforts in three broad areas:</p>
<ol>
<li><strong>Improving the data practices of organisations so that they can build and manage adequate data infrastructure and data use</strong>. Our work on&nbsp;<em>data literacy</em>&nbsp;and&nbsp;<em>data assurance</em>&nbsp;is part of these efforts.</li>
<li><strong>Tackling challenges so that the data ecosystem works better</strong>. Our&nbsp;<em>data institutions</em>&nbsp;work and our work on&nbsp;<em>data ecosystems and innovation</em>&nbsp;are part of this.</li>
<li><strong>Gathering and creating research, evidence and knowledge</strong>&nbsp;about data and the benefits of open, trustworthy data access, to inform companies and policymakers as they create data infrastructure, assets, practices and policies. This is our&nbsp;<em>evidence and foresight</em>&nbsp;work.</li>
</ol>
<p>We will continue working with our network and partners to advance our work across these areas.</p>
</div>
</section>
<h2>Using Moodle</h2>
<section>
<div>
<h4>Welcome</h4>
<p>The software we use for our courses is called Moodle, you are using it now, below is a quick guide video explaining how to use Moodle. If you have any questions or get stuck <a href="mailto:trianing@theodi.org" target="_blank" rel="noopener">get in touch with our support team.</a></p>
<div><a href="https://player.vimeo.com/video/846602928?h=be35f0ee19" target="_blank" rel="noopener">video link</a></div>
<div>
<h4>Accessibility</h4>
<p>Our course platform includes accessibility tools for an optimised learning experience. For easy access, click on the user icon in the top-right corner of the platform page and select 'Accessibility.' Here, you can enable a toolbar to personalise settings, such as font colour, spacing, and size to suit your needs. Adjust these at your convenience for a comfortable learning environment.</p>
</div>
<div>
<h4>Summary</h4>
<p>In this section we have learned:</p>
<ol>
<li>Introducing your learning environment, Moodle</li>
<li>How to navigate in Moodle</li>
<li>How to get your certificate in Moodle</li>
<li>How to access the various accessibility tools</li>
</ol>
</div>
</div>
</section>
<h2>Frequently Asked Questions</h2>
<section>
<div>
<h3>Frequently Asked Questions</h3>
What is Moodle and how does it work?
<p>To access your self-paced course on Moodle, you will need to log in to your ODI account using your username (email address) and password. Once logged in, you will find a list of all courses you have signed up to. Click on the course name to enter the course and access its contents.</p>
Can I study at my own pace in self-paced courses on Moodle?
<p>Yes, self-paced courses on Moodle are designed to allow learners to study at their own pace. You have the flexibility to progress through the course materials according to your preferred timeline.</p>
Are there any specific requirements for taking self-paced courses on Moodle?
<p>You need a computer or mobile device with internet access and a compatible web browser to access the Moodle platform.</p>
Can I start a self-paced course anytime, or are there specific start dates?
<p>Self-paced courses allow learners to start anytime they wish.</p>
How long do I have access to a self-paced course on Moodle?
<p>You are licensed to access and engage with the content of this course for 1 year from the point at which ODI launches the product to you.</p>
How do I submit assignments or assessments in self-paced courses on Moodle?
<p>Moodle provides a designated area within each course where you can submit assignments or assessments. You can upload files, enter text, or complete online quizzes depending on the specific requirements set. In some cases, ODI learning will have embedded activity types via our Adapt authoring tool. Submission instructions are typically provided within the course materials/activity.</p>
Can I interact with other learners in self-paced courses on Moodle?
<p>Interaction with other learners in self-paced courses on Moodle can vary depending on the course structure. Some courses may include discussion forums or collaborative activities where you can engage with peers. However, the level of interaction may differ. On some self-paced courses, peer-to-peer interaction is not required. Information about the courses can be found at <a href="https://learning.theodi.org/" target="_blank" rel="noopener">https://learning.theodi.org/</a></p>
How can I track my progress in a self-paced course on Moodle?
<p>Moodle offers progress tracking features to help you monitor your progress in self-paced courses. You can typically view completed activities, track grades, and check your overall progress through the course dashboard or gradebook.</p>
Are there any support services available for learners in self-paced courses on Moodle?
<p>ODI Learning is always on hand to support you with your learning experience. If your query cannot be answered via the FAQ page, please reach out to training@theodi.org and we will be glad to assist you.</p>
Can I receive a certificate upon completion of a self-paced course on Moodle?
<p>The availability of certificates of completion on our self-paced courses on Moodle are triggered based upon the completion of required activities in your course. The required completion criteria will be explained in the introduction/welcome module of your course.</p>
Are there any additional resources or external tools available for self-paced courses on Moodle?
<p>Additional resources or external tools to support your learning are referenced throughout the course.</p>
Can I retake formative and summative activities?
<p>Absolutely, we encourage learners to reattempt activities as many times as they wish to reinforce learning.</p>
How can I provide feedback or suggestions for improvement in self-paced courses on Moodle?
<p>You can typically provide feedback through the Moodle platform via survey questions we have integrated into your course. Sharing your thoughts can contribute to the enhancement of future course offerings, and we encourage feedback. Should you wish to offer feedback outside this process, please email us at training@theodi.org.</p>
I am visually impaired, how can I adjust the course interface for better accessibility?
<p>You can find accessibility options by clicking on your user icon at the top right corner of the screen and selecting 'Accessibility.' Here, you can customize settings like font color, spacing, and size.</p>
</div>
</section>
<h2>Support</h2>
<section>
<div>
<h4>Siobhan Donegan - Course Coordinator and Support</h4>
<div>
<div>Siobhan is the Learning Coordinator for ODI Learning. She runs the day-to-day platform administration for all the courses and is the main contact for students. Before joining ODI Learning, she spent 4 years at ODI HQ in the Business Support team.
<p>If you have any issues, please contact Siobhan at <a href="mailto:training@theodi.org" target="_blank" rel="noopener">training@theodi.org</a></p>
</div>
</div>
</div>
</section>
<h2>Data we collect about you</h2>
<section>
<div>
<p>We collect data about you when you sign up to complete an ODI course. We do this so we can contact you with details and to help us manage and run the course.</p>
<p>You have rights over what is done with data that is about you. You can contact us at&nbsp;<a href="mailto:datacontroller@theodi.org" target="_blank" rel="noopener">datacontroller@theodi.org</a>&nbsp;if you want to get a copy of that data. You can also ask us to update it, delete it or stop using it.</p>
<p>The data controller for this website is Open Data Institute of 5th Floor, Kings Place, 90 York Way, London N1 9AG.</p>
<p>Please send any questions about this privacy policy to&nbsp;<a href="mailto:datacontroller@theodi.org" target="_blank" rel="noopener">datacontroller@theodi.org</a>.</p>
</div>
</section>
<h2>Module overview</h2>
<section>
<div>
<h4>Building a healthy data ecosystem</h4>
<p>In order to unlock the full potential of data we must understand the data ecosystem and how to build a strong and supported data infrastructure.</p>
</div>
<div>
<h4>Learning Outcomes</h4>
<p>The aim of this module is to introduce data, why it exists on a spectrum, and how to build a strong data infrastructure to support unlocking the full value from data.</p>
<p><strong>To achieve this, you will:</strong></p>
<ol>
<ul>
<li>Define what data is, explore how it exists on a spectrum and explain how it used to construct insights and evidence for new concepts and ideas</li>
<li>Explore the history of data journalism and storytelling, and contextualise data storytelling and how you use it in your professional practice</li>
<li>Examine the concept of &lsquo;data is infrastructure&rsquo; and how this analogy enables you to understand the need for positive and sustainable investment in open data practices through the use of real-world examples</li>
<li>Explore a number of measures of success for building a healthy data ecosystem and some of the barriers that could prevent the full value of data being realised.</li>
<li>Evaluate your own practices and how data is used in your organisation</li>
</ul>
</ol>
</div>
</section>
<h2>What is data?</h2>
<p>(Scorm/Adapt Activity)</p>
<div>
<h4>What is data?</h4>
Data is a material. On its own it doesn&rsquo;t mean anything, but when we combine it with our prior experiences and prior knowledge, we are able to construct insights. These insights provide evidence for new hypotheses. From there, we make decisions on how to act on data. <strong>In this section we will explore the following: </strong>
<ul>
<li>What is data, information, knowledge and wisdom?</li>
<li>How do they differ from one another?</li>
<li>Why does knowing this difference ensure we can use data appropriately to create value?</li>
</ul>
</div>
<h2>Why does data exist on a spectrum?</h2>
<p>(Scorm/Adapt Activity)</p>
<div>
<h4>The data spectrum</h4>
<p>Data exists on a spectrum, from closed, to shared, to open. In this section, we explore these three types of data, how to go about classifying data and the role of a balancing test in making the best decision.</p>
<p>During this section consider what data you use in your professional practice: is it closed, shared or open? Are there benefits to increasing the openness of the data you use, or do the risks outweigh the benefits? Would it be inappropriate to share the data beyond the named users?</p>
<strong>In this section, we explore: </strong>
<ul>
<li>open, shared and closed data</li>
<li>the ODI Data Spectrum</li>
<li>why we need open, shared and closed data</li>
<li>how to classify your data performing a balancing test</li>
</ul>
</div>
<h2>How data has changed the way we find and tell stories</h2>
<p>(Scorm/Adapt Activity)</p>
<div>
<h4>Introduction to data storytelling</h4>
<p>In order to make data more understandable, we need to be able to present findings and insight from data. To further drive positive change, we need to be able to clearly communicate to senior level decision-makers. A key part of this is being able to tell powerful stories.</p>
<p>No field is more experienced at finding and telling stories than journalism, and likewise no field is better at using data than data science. Here, we look at what both fields can learn from each other, and what you can learn from them, in order to find and tell compelling stories with data.</p>
<strong>In this section we will explore the following: </strong>
<ul>
<li>How data has changed the way we find and tell stories</li>
<li>The history of data journalism and storytelling</li>
<li>How you use stories in your professional practice to communicate and generate buy-in</li>
</ul>
</div>
<h2>Is your data infrastructure ready?</h2>
<p>(Scorm/Adapt Activity)</p>
<div>
<h4>Data is infrastructure</h4>
<p>To truly harness the potential of data, we should regard it as a form of infrastructure. It holds equal importance to our road, electricity, and water networks, serving as a fundamental resource from which we can all benefit.</p>
<p>Data infrastructure is made up of data assets, standards, technologies, policies and the organisations that steward and contribute to them. At the ODI, we use the analogy that 'Data is infrastructure' - by that, we mean it is not a commodity, like oil. Instead, it is a necessary framework that facilitates the creation and delivery of services and products across all sectors and industries. In order for the value of data to be realised, it needs resources, investment and management.&nbsp;</p>
<p><strong>This section will introduce:</strong></p>
<ul>
<li>The concept of &lsquo;data is infrastructure'</li>
<li>What is open data?</li>
<li>What makes data open and why do we need open data?</li>
</ul>
</div>
<h2>Measuring success</h2>
<p>(Scorm/Adapt Activity)</p>
<div>
<h4>Measuring success</h4>
<p>In this module we have explored what data is, and what value it has. We&rsquo;ve examined the history and application of data journalism through the use of storytelling using Florence Nightingale and John Snow as historical examples to demonstrate how effectively sharing data and insights helps individuals and organisations to make better, more informed decisions.&nbsp;</p>
<p>One way of maximising the value of data is by opening it up and investing in data as infrastructure. In doing so, we must understand the limitations of what we can and can&rsquo;t share, but also the most appropriate ways to share it. We can use the Data Spectrum as a guide to understand what constitutes Open Data, and the Theory of Change to explore the risks associated within our professional practice.&nbsp;</p>
<p><strong>In this section, you will explore:</strong></p>
<ul>
<li>How we measure the success of data and open data</li>
<li>Your own practices and how data is used in your organisation</li>
</ul>
</div>
<h2>Barriers to success</h2>
<p>(Scorm/Adapt Activity)</p>
<div>
<h4>Barriers to success</h4>
<p>At the ODI, we have a Theory of Change. This Theory promotes collaboration and trust between organisations, governments and individuals. We want those who steward data and those who create information from this data to act in ways that lead to the best social and economic outcomes for everyone. Our Theory of Change explores: 1. the balance of trust and openness to create positive impact and 2. the dystopian scenarios that could emerge if barriers are not addressed.</p>
<p>As you are developing your understanding of the Theory of Change, consider what data you create and steward. Think about how you build trust within your professional practice, and how open you and your organisation are. How does this affect the positive outcomes you deliver?</p>
<p>This section explores:</p>
<ul>
<li>The ODI&rsquo;s Theory of Change</li>
<li>How value is created from data</li>
<li>What happens when we hoard data</li>
<li>What happens when we are fearful of data</li>
</ul>
</div>
<h2>Reflecting on your practice</h2>
<section>
<div>
<h3>Reflecting on Module 1: Building a healthy data ecosystem</h3>
<p>To support your learning we recommend taking the opportunity to reflect on the module and examine how the lessons covered can be applied to your own professional practice.</p>
<p><strong>To aid the process, we've provided a <a href="@@PLUGINFILE@@/SDS%20M1%20Building%20a%20healthy%20data%20ecosystem%20-%20Reflective%20Template.docx?time=1688722023913" target="_blank" rel="noopener">SDS Module 1 Reflective Template (.docx)</a>.&nbsp;</strong></p>
<p>The purpose of this document is to give you an opportunity to explore how your learning can apply to your professional practice. It is not an assessed piece of work. Instead, it is intended to provide a discussion framework for you and your line manager, mentor or colleagues.&nbsp;</p>
<p>We recommend that the reflections you make during this course are used as part of a portfolio of learning and development, used in conjunction with your own professional development frameworks.</p>
</div>
</section>
<h2>Module summary</h2>
<section>
<div>
<h3>Summary: Building a healthy data ecosystem</h3>
In order to unlock the full potential of data we must understand the data ecosystem and how to support and build a strong and supported data infrastructure. Open data plays a key role in this ecosystem.&nbsp;Open data maximises innovation and opportunities for unexpected&nbsp;benefit. Open data is data that anyone can access, use and share. Open data is a raw material for the digital age but, unlike coal, timber or diamonds, it can be used by anyone and everyone at the same time. Any restrictions imposed on the use of open data will limit its potential for creating new value. Open data can bring diverse benefits to governments, businesses and individuals. It has the power to help improve services, grow economies and protect our planet.<strong>In this module we explored: </strong>
<ul>
<li>Data: what it is, why it exists on a spectrum and how it is used to construct insights and evidence for new concepts and ideas</li>
<li>Data journalism and storytelling, contextualised data storytelling and how you use it in your professional practice &lsquo;</li>
<li>Data as infrastructure and how this analogy enables you to understand the need for positive and sustainable investment in data</li>
<li>Measures and barriers to success and a theory of change for data</li>
<li>Finally, you have reflected on your own professional practice and how you tell stories and create value</li>
</ul>
<p>There are many places we could go from here; a strong data infrastructure needs to be supported with:</p>
<ul>
<li>A clear strategy, governance and ethical framework</li>
<li>Experts who maintain the infrastructure for the benefit of everyone</li>
<li>A community of users and innovators who build on top of the data infrastructure</li>
</ul>
<h5>Upcoming Modules: The ODI Data Skills Framework&nbsp;</h5>
<p>The Data Skills Framework shows the broad range of skills required to get the most from data.&nbsp;In this course we will be exploring many areas of the framework, including:</p>
<ul>
<li>How to organise, structure and clean data</li>
<li>How to start analysing data to find insight</li>
<li>How to visualise and tell a story from data</li>
<li>What the future of data platforms looks like</li>
<li>The role of data ethics to minimise harmful impacts</li>
</ul>
<p>In &lsquo;Module 2: Organising and Validating Data&rsquo; you will apply data skills to clean data and increase its usability.</p>
</div>
</section>
<h2>Module overview</h2>
<section>
<div>
<p>In Module 1, you explored what data is, and how it can be used to create value. Value is recognised by stakeholders when we communicate the key insights and findings in the data. To ensure we maximise that value, we need to ensure that data is properly gathered, organised, structured and cleaned.</p>
<p>Module 2: 'Organising and Validating Data' introduces the skills to perform these tasks.</p>
</div>
<div>
<h4>Learning outcomes</h4>
<p>By the end of this section you will understand how to organise and clean data, as well as undertake activities to quality check and enable greater utility and interoperability of data.</p>
<p><strong>To achieve this, you will:</strong></p>
<ul>
<li>Investigate the gathering of data, including various sources of data and the practices employed in finding the data</li>
<li>Examine the structure of data and determine approaches to organise the data into the appropriate structure as well as apply cleaning techniques to increase the utility of data</li>
<li>Understand the purpose of data schemas and how to create and use them</li>
</ul>
</div>
</section>
<h2>The rising value of data</h2>
<section>
<div>
<h4>Data: Gathering, organising, structuring and cleaning&nbsp;</h4>
<p>The value of data increases as it turns into a story. This is why data science and journalism skills must be used cooperatively. Those who work with the data must be able to communicate why it is important or useful, otherwise the value is lost.</p>
<p>Creating value from data is a sequence. This sequence repeats in a cycle and follows the order: <strong>Gathering, Organising, Structuring and Cleaning.</strong></p>
<p><strong>The following video explains why this is important.</strong></p>
<div><a href="https://player.vimeo.com/video/833396339?h=932ac95d46&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" target="_blank" rel="noopener">video link</a></div>
<p>In the next section we introduce the skills and knowledge involved in gathering data.</p>
</div>
</section>
<h2>Gathering data</h2>
<p>(Scorm/Adapt Activity)</p>
<div>
<h4>Gathering data</h4>
Data plays a key role in storytelling, however not all data is easy to find. The growing demand for data has led to an increase in human friendly data services, including data portals and simple download buttons. However, downloadable data represents only a small fraction of the available data on the Web. The majority of data available on the Web is hidden from the human eye. But machines can find and read this data.<strong>In this section we explore the following: </strong>
<ul>
<li>The difference between downloadable and hidden data</li>
<li>Finding downloadable data</li>
<li>Finding hidden data</li>
<li>The benefits of using hidden data</li>
</ul>
</div>
<h2>Organising and structuring data</h2>
<p>(Scorm/Adapt Activity)</p>
<div>
<h4>Organising and Structuring data</h4>
Once we have gathered data, we need to ensure it is usable. A common challenge in ensuring data quality is difficulties people face when using spreadsheets. When data is properly managed, it is much easier to answer fundamental questions and conduct analysis. Correctly managed data also ensures you aren't making decisions based on faulty evidence. Knowing how to structure and organise data in a spreadsheet is fundamental to ensuring consistency in your data. In this section we look at how to effectively structure a spreadsheet for raw data collection. <strong>We will cover: </strong>
<ul>
<li>Spreadsheet layouts</li>
<li>Column titles</li>
<li>Header rows and the freeze function</li>
<li>Data types</li>
</ul>
At the end of this section you can try out what you've learnt on a spreadsheet.</div>
<h2>Choosing the right structure and format for data</h2>
<p>(Scorm/Adapt Activity)</p>
<div>
<h4>Choosing the right structure and format for data</h4>
How should data be structured? Who needs to consume the data - humans, machines or both? So, how should data be structured? Choosing the right format helps ensure the data can be simply managed and reused. To maximise reuse of data, it may be necessary to use a number of structures and formats available across different platforms to suit users' needs. <strong>In this section we'll explore: </strong>
<ul>
<li>The difference between machine and human-readable data</li>
<li>How to create machine-readable tabular data</li>
<li>Other data structures and formats</li>
<li>How to choose the right structure and format</li>
</ul>
</div>
<h2>Cleaning data</h2>
<p>(Scorm/Adapt Activity)</p>
<div>
<h4>Cleaning data</h4>
One of the biggest challenges when working with any data is dealing with errors. Often errors are not even noticed by data publishers because the data can change over many years. In other cases, errors can be the result of human mistakes in data entry, like mistyping or incorrect abbreviations. When working with any data, it is important to know how to find errors and correct them to make the data more useful. <strong>In this section we'll explore the following: </strong>
<ul>
<li>Common data errors</li>
<li>Useful data cleaning tools</li>
<li>Reasons for cleaning data</li>
</ul>
</div>
<h2>[Optional] Getting started with Open Refine</h2>
<section>
<div>
<h4>Open Refine</h4>
<p>Whenever you want to carry out analysis, you should always take the time to inspect the data you have been given to see if there are any errors like misspelling, repeated entries, mixed numerical scales and mixed ranges, as well as data that might be missing. Otherwise you run the risk of transferring these mistakes into your analysis.</p>
<p>Open Refine is a powerful desktop app used for re-organising and cleaning data.&nbsp;</p>
<h4>Watch the video below to get an introduction to Open Refine.</h4>
<h3>Installing Open Refine</h3>
<p>To get started with this exercise you will require a copy of Open Refine which is available from <a href="http://openrefine.org/download.html" target="_blank" rel="noopener">openrefine.org</a>.</p>
<p>For Windows, Refine comes as a pre-built .EXE file that you simply run and does not require installing.</p>
<p>Once running, Refine will create a web server on your local machine that you interact with via the URL <a href="http://127.0.0.1:3333" target="_blank" rel="noopener">http://127.0.0.1:3333</a>.</p>
<h3>OpenRefine Community Support</h3>
<p>If you require support in using OpenRefine, there is an active community available through their <a href="https://forum.openrefine.org/" target="_blank" rel="noopener">website forum</a>.</p>
<p>OpenRefine also maintain a comprehensive <a href="https://openrefine.org/docs" target="_blank" rel="noopener">user manual</a>.</p>
</div>
</section>
<h2>Hands-on: Organising and structuring data</h2>
<p>(Scorm/Adapt Activity)</p>
<section>
<div>
<h4>Applying your skills and knowledge</h4>
<p>Throughout this module you have been building your skills in the gathering, organising, structuring and cleaning of data. These skills are transferable across all different types of data, including the data you use at the Wildlife Trust.&nbsp;</p>
<p>The following assignment has been designed to allow you to apply your skills in organising, structuring and cleaning a dataset.&nbsp;</p>
<p><strong>The assignment requires you to:</strong></p>
<ul>
<li>Reorganise and restructure a messy dataset into a single dataset that can be analysed simply by both human and machine.</li>
<li>Create a single CSV file (you can export this from tools like Excel) that contains just the dataset.</li>
<li>The dataset should contain enough data to enable analysis of the data by country, type, species and ownership.&nbsp;</li>
</ul>
<p><strong>Where is the data?</strong></p>
<p>You can download the dataset <a href="@@PLUGINFILE@@/WLT%20exercise%20dataset.xlsx" target="_blank" rel="noopener">here</a>.</p>
<p><strong>What tools can I use?</strong></p>
<p>You can make use of any tools you are comfortable with, however the submission must be a CSV file.</p>
<p><strong>How will I receive feedback?</strong></p>
<p>Once you have completed your assignment you should upload the dataset via the submission portal below. Providing that you submit a valid CSV file, our AI assistant tutor will attempt to assess your submission and give customised feedback.&nbsp;</p>
<p>The AI assistant tutor uses AI that we have specifically trained to deal with this task. It has been given an example, model answer and some other marking criteria. The assistant tutor will look to mark against four criteria:</p>
<ol>
<li>How the data is structured</li>
<li>The consistency of the data in each row/column</li>
<li>The cleanliness of the data</li>
<li>The ability for a machine to reuse the data</li>
</ol>
<p><strong>IMPORTANT</strong>: We do not expect the assistant tutor to be 100% reliable and accurate. We have applied it here in order to examine the ability for AI to give instant feedback rather than wait for a human tutor to be available. We believe there is some value in doing this, but also recognise the need for a human to be 'in-the-loop'. For this reason we are building in checks and processes that allow a human tutor to override the AI assistant tutor. We are also building in ways for you to report your feedback to us so it can be flagged and we can use the learning to improve (or remove) the assistant tutor. Think of them as being on probation.</p>
</div>
<div>
<h4>Submit your answer via the link below.</h4>
</div>
</section>
<h2>Choosing and designing schemas</h2>
<p>(Scorm/Adapt Activity)</p>
<div>
<h4>Choosing and designing schemas</h4>
<p>Data schemas are structured definitions or models that specify the structure, format, and constraints of data within a particular system or database. They define the organisation and expected characteristics of the data, including the data types, relationships, and rules that govern the data. Data schemas provide a blueprint for how data should be structured and stored, ensuring consistency and integrity.</p>
<p>While they provide similar features to data validation in Excel, data validation rules in Excel are primarily applied to individual cells or ranges and focus on immediate data validation within the spreadsheet.&nbsp;However, they do not provide a comprehensive and standardised structure for an entire dataset or database.<strong>In this section we explore: </strong></p>
<ul>
<li>What is a schema?</li>
<li>Why do schemas matter?</li>
<li>Schemas in practice</li>
<li>Choosing or designing schemas</li>
</ul>
</div>
<h2>Hands-on: Creating schemas and validating data</h2>
<p>(Scorm/Adapt Activity)</p>
<section>
<div>
<h4>Applying your skills and knowledge</h4>
<p>So far in this module we have explored the creation of well structured, machine readable data that conforms to a number of schemas. In the last part of the module, we have put together a tool that allows you to explore how to create schemas for yourself to validate any data. You can either pick the dataset from the previous exercise, or a dataset of your own which you can create a schema for and find out if you can validate the data.&nbsp;</p>
<p>In order to give you something to take away, we will also provide you with the JSON schema you can use with other tools.</p>
<p><strong>The assignment requires you to:</strong></p>
<ul>
<li>Identify a dataset you want to create a schema for</li>
<li>Create a schema that can be used to verify your data</li>
<li>Download the schema for sharing with others</li>
</ul>
<p><strong>Where is the data?</strong></p>
<p>For this exercise you can use your own data, however be aware that the tool is limited to accepting data in a single CSV file.</p>
<p><strong>What tools can I use?</strong></p>
<p>You can make use of any tools you are comfortable with, however the submission must be a CSV file.</p>
<p><strong>How will I receive feedback?</strong></p>
<p>Once you have completed your assignment you should upload the dataset via the submission portal below. Providing that you submit a valid CSV file and have created a good schema, it should validate and pass the activity.&nbsp;</p>
<p>There is an easy way to pass this exercise. Create a schema that accepts any input. For this reason our AI assistant tutor will attempt to assess your submission and give customised feedback on how the schema could be improved.</p>
<p>The AI assistant tutor uses the data you upload as well as the schema to provide feedback and potentially a new schema.&nbsp;</p>
</div>
<div>
<h4>Submit your answer via the link below.</h4>
Launch submission portal
<p><em>If you are viewing this page using the Moodle app,&nbsp;<a href="https://odi-test.opensourcelearning.co.uk/mod/scorm/player.php?a=367&amp;scoid=734&amp;currentorg=adapt_scorm&amp;mode=&amp;attempt=1" target="_blank" rel="noopener">click here</a>&nbsp;to launch the e-learning</em></p>
<div>
<div>CLOSE AND RETURN TO COURSE</div>
<div>Loading...</div>
</div>
</div>
</section>
<h2>Reflecting on your practice</h2>
<section>
<div>
<h3>Reflecting on Module 2: Organising and structuring data</h3>
<p>To support your learning we recommend taking the opportunity to reflect on the module and examine how the lessons covered can be applied to your own professional practice.</p>
<p><strong>To aid the process, we've provided a <a href="@@PLUGINFILE@@/SDS%20Module%202%20Reflective%20Template.docx" target="_blank" rel="noopener">SDS Module 2&nbsp;Reflective Template (.docx)</a>.&nbsp;</strong></p>
<p>The purpose of this document is to give you an opportunity to explore how your learning can apply to your professional practice. It is not an assessed piece of work. Instead, it is intended to provide a discussion framework for you and your line manager, mentor or colleagues.&nbsp;</p>
<p>We recommend that the reflections you make during this course are used as part of a portfolio of learning and development, used in conjunction with your own professional development frameworks.</p>
</div>
</section>
<h2>Module summary</h2>
<section>
<div>
<h4>Summary: organising and validating data</h4>
<p>This module has introduced the skills required to ensure we maximise the value of data through the proper gathering, organising, structuring and cleaning. This is step one of the four step process of storytelling with data, and is likely to be a step where you spend much of your time.</p>
<p>By spending the appropriate amount of time organising and validating our data, we can ensure that the outputs that our stakeholders receive is as effective and valuable as possible.&nbsp;</p>
<p>In this module you have undertaken activities that quality check and enable greater utility and interoperability of data. This has included you:</p>
<ul>
<li>Examining the structure and organising the data into the appropriate structure as well as apply cleaning techniques to increase the utility of data</li>
<li>Building an understanding of&nbsp; the purpose of data schemas and how to create and use them, further supporting your approaches to organising data effectively</li>
<li>Applying these skills to different datasets and contexts to explore the transferable nature of these practices across the different datasets you use in your professional practice</li>
</ul>
<p>In module 3 we introduce data analysis and explore the skills applied in establishing the shape of data, predicting trends and appropriately filtering data in order to start identifying key insights and building your data story.</p>
</div>
</section>
<h2>Module overview</h2>
<section>
<div>
<p><em><strong>"There are three kinds of lies: lies, damned lies, and statistics."</strong></em></p>
<p>Benjamin Disraeli</p>
<p>Having spent a lot of time gathering and preparing data, the temptation is to immediately begin to analyse the data using any technique that seems relevant. First, however it is important to explore the data to discover its shape and any limitations it might have. We also need to understand the benefits and risks of applying each statistical technique to data.</p>
<p>So before we perform any analysis, this module looks at three key things:</p>
<ol>
<li>How numbers and summary statistics can lead us to incorrect conclusions in "Seeing through a world of numbers"</li>
<li>Why establishing "the shape of data" is critical to discover its limitations and biases.</li>
<li>Why plotting "Trends in data" is more challenging than taking a ruler to a plot.</li>
</ol>
<p>Following this we'll then look at how some key statistical techniques and tools can be applied to data.</p>
<p>In this module we'll be analysing the performance of the London Fire Brigade and look at the impact on the service of closing 10 fire stations in London in 2014 in order to cut back on spending. Was it a good idea? What has happened since? Is the performance good or bad? Is it fair? What else should happen now?&nbsp;</p>
<p>Module 3 introduces the skills to perform these tasks in order to see what can be discovered from the data.</p>
</div>
<div>
<h4>Learning Outcomes</h4>
<p>By the end of this section you will have the skills to evaluate data from different sources in order to establish its quality, and carry out exploratory data analysis to produce insights.</p>
<p><strong>To achieve this, you will:</strong></p>
<ul>
<li>Evaluate the benefits and risks of applying different statistical techniques to data</li>
<li>Apply several exploratory data analysis techniques to create summary statistics and visual representations</li>
<li>Practise the skills to establish the shape of data and examine what it means, and how it can be used to determine the state of data</li>
<li>Use tools to filter and explore trends in data</li>
<li>Evaluate the difference between qualitative and quantitative data analysis</li>
</ul>
</div>
</section>
<h2>Refining, observing, analysing and calculating</h2>
<section>
<div>
<p>Once we have collected data that is relevant to our projects, we need to then employ several skills to filter that data and make it usable for our purpose and objective.&nbsp;</p>
<p>The steps to determine the state and quality of the data are:</p>
<ol>
<li>Refine,</li>
<li>Observe,</li>
<li>Analyse, and&nbsp;</li>
<li>Calculate.&nbsp;</li>
</ol>
<p>The following video explains why this is important.</p>
<div><a href="https://player.vimeo.com/video/837962894?h=ea188c18b4&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" target="_blank" rel="noopener">video link</a></div>
<p>In the next section we introduce the power of numbers, and the skills required to see through the issues and establish accurate and appropriate insights.</p>
</div>
</section>
<h2>Seeing through a world of data</h2>
<p>(Scorm/Adapt Activity)</p>
<div>
<h4>Seeing through the numbers</h4>
<p>Public spending, health risks, environmental disasters, who is rich, who is poor, pensions, the best and worst schools and hospitals, immigration &ndash; life comes in numbers. The trick is seeing through them.</p>
<p>To some, numbers can be intimidating. They can also create illusions and lead us to see what is not actually there. If numbers are not understood then how can we possibly understand what conclusions we can draw from them and the impact this has on people!</p>
</div>
<h2>The shape of data</h2>
<p>(Scorm/Adapt Activity)</p>
<div>
<h4>The shape of data</h4>
<p>Data, it turns out, has shape. That shape has meaning.</p>
<p>The shape of data tells you everything you need to know about your data from its obvious features to its deepest secrets.</p>
<p>Establishing the shape and meaning of data prior to using it in analysis is essential to avoid basic mistakes that can lead to harmful impacts.</p>
</div>
<h2>Trends in data</h2>
<p>(Scorm/Adapt Activity)</p>
<div>
<h4>Trends in data</h4>
<p>One of the key benefits of data is that of finding insight, spotting patterns and being able to make predictions. However, there are risks with mixing descriptive statistics, used for summarising existing data, and inferential statistics which focuses on drawing conclusions, making predictions, or generalising from a sample to a larger population.</p>
<p>This module explores:</p>
<ul>
<li>The difference between descriptive and inferential statistics</li>
<li>Inferential statistics applied to data</li>
<li>How to avoid overfitting</li>
<li>Working with confidence</li>
</ul>
</div>
<h2>Filtering and pivot tables</h2>
<p>(Scorm/Adapt Activity)</p>
<div>
<h4>Filtering and pivot tables</h4>
<p>At this point, we have explored the organising and cleaning of data, and we've looked at exploratory data analysis - what it is and how it helps us to start to analyse data. Through this initial analysis we are also identifying which parts of the data are most useful, and which are not useful at all. We can now start to filter our data and set aside anything that is out of scope for our current objectives. Through filtering the data we can start to identify trends in the data.&nbsp;</p>
<p>In these exercises we make use of core functionality that is available in most spreadsheet applications.&nbsp;</p>
</div>
<h2>Analysing the performance of the London Fire Brigade</h2>
<p>(Scorm/Adapt Activity)</p>
<div>
<h4>Analysing the performance of the London Fire Brigade</h4>
In this module you will be focusing on data analysis through the lens of the London Fire Brigade's operations and the impact of the 2014 station closures. You will be guided through real-world data, enhancing your analytical skills and your ability to interpret and present complex information.
<h6><strong>Backstory</strong></h6>
<p>In 2013, it was announced that cuts to the London Fire Brigade meant that ten fire stations in the capital had to be closed. These closures happened a year later in 2014, but what actually happened as a result of the closures? What more could be done to improve the efficiency and overall performance of the London Fire Brigade? Where is the data?</p>
<p>Below, we provide you with a dataset directly from the London Fire Brigade which has been released under an open license.&nbsp;</p>
<p>If you wish, you should be able to find several additional relevant datasets online with a simple search, and you can use the following questions to support you in deciding which data to use:</p>
<ul>
<li>What data is available?</li>
<li>How usable is the data?</li>
<li>Which fire stations closed?</li>
<li>When did the fire stations close?</li>
<li>What was the performance before these closed?</li>
<li>Are there any data collection techniques or targets that influence the data?</li>
</ul>
There may be many more questions that you might need to answer to inform your analysis.
<p>In addition, the <a href="http://london-fire.labs.theodi.org/map.html" target="_blank" rel="noopener">Open Data Institute built a tool</a> that predicted the impact of closing the proposed fire stations, which you may find useful to explore.</p>
<p>As you work through this exercise, think about the different contexts in which you might apply these skills. The analysis undertaken in this section is transferable and is applicable across all different types of datasets.</p>
<div>
<h4>Important</h4>
<p><strong>Before you get started, please <a href="@@PLUGINFILE@@/LFB-data.csv" target="_blank" rel="noopener">download this dataset</a>.&nbsp;</strong></p>
</div>
</div>
<h2>Analysing charitable donations to the Wildlife Trust</h2>
<section>
<div>
<h4>Applying your skills and knowledge</h4>
<p>Now you have the opportunity to explore a dataset that is more recognisable. In this exercise, we provide you with data relating to one-off donations made to Trusts in South Yorkshire and the East Midlands.&nbsp;</p>
<p>This is an opportunity to apply what you've learnt about exploratory data analysis. During this process you make observations and generate hypotheses. You are seeking to identify patterns and spot anomalies, and ultimately build evidence to support your hypothesis.&nbsp;</p>
<p>In this exercise you will use the following dataset: <a href="@@PLUGINFILE@@/Analysing%20Charitable%20Donations%20%282%29.xlsx" target="_blank" rel="noopener">Analysing Charitable Donations</a>.xlsx</p>
<p><strong>Note: </strong>the dataset is synthetic and does not refer to real people or detail any real donations made.&nbsp;</p>
<p>The dataset contains data on donations made to Wildlife Trusts in South Yorkshire and the East Midlands, all made during August 2022. Work with this data and try to identify the following:</p>
<ul>
<li>The trust with the highest number of donations</li>
<li>The median donations of all trusts</li>
<li>Which trust receives donations of the highest value</li>
<li>The modal value for each trust</li>
</ul>
</div>
</section>
<h2>Introduction to qualitative data analysis</h2>
<section>
<div>
<h3>Introduction to Qualitative Data Analysis</h3>
<p>Not all data comes in the form of structured tables or accurately located geographic data. Often it can be qualitative data that is the hardest to analyse and work with. Qualitative data is information that cannot be measured and is very subjective as a result. Even colour can be subjective.</p>
<p>The aim of qualitative data analysis is to reduce and make sense of vast amounts of information, often from different sources, and to offer an explanation, interpretation or thematic summary of the data. Inputs to qualitative analysis can take many forms including interview transcripts, documents, blogs, surveys, pictures or videos.</p>
<p>Qualitative data analysis is a more natural process for humans who naturally seek to distil inputs into themes and key outcomes, as is especially true of meetings or focus groups. People will often use mind-mapping or post-it based thought maps to help group together and categorise wide-ranging discussion into key themes.</p>
<p>Qualitative data analysis ought to pay attention to the spoken word&rsquo;s context, consistency and contradictions of views, frequency and intensity of comments, their specificity as well as emerging themes and trends.</p>
<p>There are two main ways of analysing qualitative data, <strong>framework analysis</strong> and<strong> thematic network analysis</strong>.</p>
</div>
<div>
<h3>Framework Analysis</h3>
<p>Framework analysis involves building a predefined set of criteria that clearly reflect your aims, objectives and interests. Using this set of criteria, the relevant pieces of information can be extracted from the data and compared with other inputs in the framework. Using a framework allows many researchers to do the extraction while minimising the chances of qualitative analysis bias. A bias can still be introduced at framework design stage which may result in key information being missed.</p>
<p>An alternative approach is to not construct a framework, but rather to apply thematic network analysis. Thematic network analysis is a more exploratory approach which encourages the analysis of all of the input data which can shape the output in unexpected directions.</p>
<p>In reality, the majority of qualitative data analysis will involve a combination of the two approaches.</p>
<p>Whichever approach is chosen, the first step in any qualitative data analysis involves familiarisation with the data; reading and re-reading responses. At the same time, it is a good idea to start codifying the data by writing down keywords and topics which attempt to reduce and interpret the data. The result of the coding process could be the thematic network analysis or a framework by which all responses need to be coded. Either way, coding can be a long, slow and repetitive process, however there are a number of tools that can help with thematic network analysis.</p>
</div>
<div>
<h3>Thematic Network Analysis</h3>
<p>Entity recognition tools provide one such technique that can help analyse and enrich qualitative data. Essentially, entity recognition seeks to locate and classify named entities in text into predefined categories such as the names of persons, organisations, locations, expressions of times, quantities, monetary values, percentages, etc.</p>
<p><em><em>The&nbsp;<a href="https://permid.org/onecalaisViewer" target="_blank" rel="noopener">Intelligent Tagging Service</a></em>(shown below) tagging of <a href="http://www.bbc.co.uk/news/uk-northern-ireland-41434078" target="_blank" rel="noopener">BBC News: NI farms 'can expect same support' post-Brexit - Gove</a></em></p>
<p><em>The&nbsp;<a href="https://permid.org/onecalaisViewer" target="_blank" rel="noopener">Intelligent Tagging Service</a>, provided by Refinitiv (formerly known as Calais from Thomson Reuters), is one such example of an entity recognition engine. More than just entity recognition, the Calais service links those entities to data records that exist within the permID database of entities and financial records.</em></p>
<p>More widely, such entity recognition is used by services such as <a href="https://www.theyworkforyou.com/" target="_blank" rel="noopener">TheyWorkForYou</a> in order to track the activities of politicians and provide this in an easy way to the public.</p>
<p>Calais is a good example of an online tool that can perform entity recognition and provide links into additional data on each topic. Other techniques can be much simpler but just as effective, such as <a href="https://www.wordclouds.co.uk/" target="_blank" rel="noopener">word cloud generators</a>.</p>
</div>
</section>
<h2>Reflecting on your practice</h2>
<section>
<div>
<h3>Reflecting on Module 3: Introduction to data analysis</h3>
<p>To support your learning we recommend taking the opportunity to reflect on the module and examine how the lessons covered can be applied to your own professional practice.</p>
<p><strong>To aid the process, we've provided a <a href="@@PLUGINFILE@@/SDS%20Module%203%20Reflective%20Template.docx" target="_blank" rel="noopener">SDS Module 3 Reflective Template (.docx)</a>.&nbsp;</strong></p>
<p>The purpose of this document is to give you an opportunity to explore how your learning can apply to your professional practice. It is not an assessed piece of work. Instead, it is intended to provide a discussion framework for you and your line manager, mentor or colleagues.&nbsp;</p>
<p>We recommend that the reflections you make during this course are used as part of a portfolio of learning and development, used in conjunction with your own professional development frameworks.</p>
</div>
</section>
<h2>Module summary</h2>
<section>
<div>
<h4>Improving performance of data&nbsp;</h4>
<p>The exploration of data is an important step in maximising the utility of our data. Once gathered, we filter the data to ensure it is appropriate for our purposes. We may remove parts of the data set that are superfluous to our needs, or we may fix data quality issues. But in order to do this, we must explore the data to find out more.</p>
<p>The exploration of data involves the practice of applying data analysis techniques and the creation of summary statistics and visual representations. Through this practice we can establish the shape of data. The shape of data tells us everything we need to know about data from the obvious to the hidden.&nbsp;</p>
<p>Through this module you should now be confident in performing exploratory data analysis activities, such as:</p>
<ul>
<li>Creation of summary statistics and visual representations</li>
<li>Establishing the shape of data</li>
<li>Using tools to filter data and explore trends</li>
</ul>
<p>In module 4 we continue building on the skills that improve the performance of data and use our findings from exploratory data analysis to start creating effective visualisations.&nbsp;</p>
</div>
</section>
<h2>Module overview</h2>
<section>
<div>
<h4>&nbsp;Module 4 overview</h4>
<div>
<p>The course so far has focused on working with data directly to clean, structure and analyse the data. In this module, we change our focus to data visualisation.&nbsp;</p>
<p>We present data to our stakeholders by telling stories. To accompany the story, we produce visualisations that support the explanation of the scale and impact of our data.</p>
</div>
</div>
<div>
<h4>Learning outcomes</h4>
<p>By the end of this section you will understand how to produce and present appropriate data visualisations that deliver a connected and relevant narrative.&nbsp;</p>
<p>To achieve this, you will:</p>
<ul>
<li>Explore data visualisation best practice and understand the principles behind effective data visualisations</li>
<li>Examine where visualisations can be used to misrepresent or deceive and the impact this has on our outcomes</li>
<li>Evaluate complex stories and their impact</li>
<li>Create a narrative that tells the story of the data</li>
</ul>
</div>
</section>
<h2>Rising value to the public</h2>
<section>
<div>
<h3>Data Visualisation</h3>
<p>When effectively analysed and presented in a clear and compelling way, data has the potential to create impact. Whether that&rsquo;s changing perceptions, offering counterintuitive insights or prompting action, impact happens when data acts as the catalyst for change. As we work through the stages of rising value, we gather our data and clean it, following that with filtering and analysis.</p>
<p>As per the Pareto Principle (the 80:20 rule), this is where we will have spent much of our time. But, at the heart of driving change is the skill of finding and telling stories using, where relevant, compelling visualisations. A modern data scientist will work with many different types of visualisation, from common graphs to advanced statistical plots to geographic representations. It is the elements of visualisation and storytelling that our stakeholders care about, because it&rsquo;s the elements they will see.&nbsp;</p>
<p><strong> </strong></p>
</div>
</section>
<h2>Communicating outputs</h2>
<p>(Scorm/Adapt Activity)</p>
<div>
<h4>Communicating outputs</h4>
<div>
<p>There&rsquo;s something breathtaking about data that is communicated well &mdash; it&rsquo;s a lot like marvelling at an architectural masterpiece. Think about how you felt last time you looked at an engineering or architectural masterpiece, how did it make you feel? Did you feel elated? Motivated? Educated?</p>
<p>Data communicated well can do exactly the same. Alternatively, when not done well you end up suffering a long presentation of poorly presented, cluttered and misleading charts that don&rsquo;t seem to be relevant to your problem.</p>
<p>Find out more in this eLearning module about how to start creating an inspiring visual story from data.</p>
</div>
</div>
<h2>Data visualisation formats</h2>
<p>(Scorm/Adapt Activity)</p>
<div>
<h4>Data visualisation formats</h4>
<p>Visualisations have the power to distil complex data sets into clear and concise stories. Used properly, they have the ability to condense a huge amount of information into a compact and easily digestible form.&nbsp;&nbsp;</p>
<p>The human brain is adept at recognizing and interpreting visual patterns. Stimulating our visual senses and leveraging their natural inclination towards visual stimuli, data visualisations are far more likely to capture attention and be remembered compared to textual or numerical data. By carefully crafting the design and layout, visualisations can guide viewers through a narrative, presenting powerful stories that are engaging and memorable.</p>
<p>In this module we are going to cover:</p>
<ul>
<li>The data visualisation toolkit</li>
<li>Best practices for creating impact with different types of data visualisation</li>
<li>Using infographics</li>
<li>Adding interactivity</li>
<li>Selecting the right visualisation format</li>
</ul>
</div>
<h2>Fundamentals of visual perception and design</h2>
<p>(Scorm/Adapt Activity)</p>
<div>
<h4>Fundamentals of visual perception</h4>
<p>Data Visualisations come in all shapes and sizes, they can be simple and they can be incredibly complex, but they should always communicate the message in an understandable way. In this section we introduce the underlying science of how the visual cortex works and how you can use that knowledge to create effective visualisations.&nbsp;</p>
<p>This forms the basis of data visualisation best practices. These can then be applied to maximise the communication of your key messages and insights.</p>
</div>
<h2>Visual deception</h2>
<p>(Scorm/Adapt Activity)</p>
<div>
<h4>Visual deception</h4>
<p>The use of visual deception can create intrigue, but it can also be used to mislead. Here, we examine examples of visual deception and the complexities they create when communicating a message. As you learn, consider the data practices we explored in modules 1, 2 and 3, and how they might contribute to misleading information.</p>
<p>A modern data scientist should be able to choose deception techniques that engage and inform, and avoid those which intentionally mislead.</p>
</div>
<h2>Turning a visualisation into a story</h2>
<p>(Scorm/Adapt Activity)</p>
<div>
<h4>Turning a visualisation into a story</h4>
<p>Data visualisations are a powerful way of bringing data to life. They help convey messages quickly and succinctly. However on their own they can lack context and the risks of misinterpretation are high.&nbsp;</p>
<p>To add context to the visualisation, try to build a strong story with clear headline and narrative.&nbsp;</p>
<p>This module goes through the steps you should take in putting your story into words for the best possible impact</p>
</div>
<h2>Telling the story of the London Fire Brigade</h2>
<p>(Scorm/Adapt Activity)</p>
<div>
<h4>Telling the story of the London Fire Brigade</h4>
In the last module we asked you to analyse some data published by the London Fire Brigade, covering a short period before and after several stations were closed in 2014. For the next exercise we are revisiting that topic in the context of data storytelling; how would you build an engaging narrative using this data? Think about the concepts we have covered in this module, and see if you can apply those principles to select the most appropriate options from the choices below</div>
<h2>Reflecting on your practice</h2>
<section>
<div>
<h3>Reflecting on Module 4: Telling stories with data</h3>
<p>To support your learning we recommend taking the opportunity to reflect on the module and examine how the lessons covered can be applied to your own professional practice.</p>
<p><strong>To aid the process, we've provided a <a href="@@PLUGINFILE@@/SDS%20M4%20Telling%20stories%20with%20data%20-%20Reflective%20Template%20%282%29.docx" target="_blank" rel="noopener">SDS Module 4 Reflective Template (.docx)</a>.&nbsp;</strong></p>
<p>The purpose of this document is to give you an opportunity to explore how your learning can apply to your professional practice. It is not an assessed piece of work. Instead, it is intended to provide a discussion framework for you and your line manager, mentor or colleagues.&nbsp;</p>
<p>We recommend that the reflections you make during this course are used as part of a portfolio of learning and development, used in conjunction with your own professional development frameworks.</p>
</div>
</section>
<h2>Module summary</h2>
<section>
<div>
<h4>Telling stories with data</h4>
<p>Through this module you should now be confident in working with your data to create effective and appropriate data visualisations by:</p>
<ul>
<li>Understanding the principles behind data visualisation best practices</li>
<li>Examine data visualisations and determine whether or not they are deceiving or misleading</li>
<li>Evaluate complex stories and their impact</li>
<li>Create a narrative that tells the story of data</li>
</ul>
<p>Throughout these modules we have been looking at how to use data to help inform decision making. But who benefits from the decisions being made? How do we ensure that we are not just compliant but also ethical? The next module introduces the critical area of data ethics and how to start operationalising it within projects.</p>
</div>
</section>
<h2>Module overview</h2>
<section>
<div>
<h4>&nbsp;Module 5 overview</h4>
<div>
<p>All decisions made have an impact on someone. Increased responsibility is being placed on data and algorithms that process it to automate, speed up, or make decisions at scale. In everything from provision of healthcare, allocation of benefits and credit to predicting crime and even deciding how to optimise the performance of emergency services. Data informed decision making is everywhere. But are the benefits fair? Are people protected from the harmful impacts? Who decides?&nbsp;</p>
<p>At the Open Data Institute (ODI), we advocate for and support practices that increase trust and trustworthiness: building ethical considerations into how data is collected, managed and used; ensuring equity around who accesses, uses and benefits from data; engaging widely with affected people and organisations. In using data, the full extent of the consequences have not always been explored.&nbsp;</p>
<p>Find out more in the video below.</p>
<div>
<div>
<div><a href="https://player.vimeo.com/video/833616283?h=bff9fc1a9f&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" target="_blank" rel="noopener">video link</a></div>
</div>
</div>
<p>In this section we will be exploring data ethics and the tools available to help everyone ensure that the monitoring of ethics is more akin to compliance monitoring as opposed to a review that gets done at the start of the project and forgotten. Ensuring ethics is at the centre of a project helps deliver successful outcomes people trust (even if they don't agree with them).</p>
</div>
</div>
<div>
<h4>Learning Outcomes</h4>
<div>
<p>By the end of this section, you will gain the skills to evaluate data's impact, identify data ethics concerns, and generate recommendations to mitigate negative effects.</p>
<p><strong>To achieve this, you will:</strong></p>
<ul>
<li>Explore the concept of data ethics</li>
<li>Examine examples of unethical data practices</li>
<li>Apply the consequence scanning toolkit to evaluate the impact of real-world data ethics case studies</li>
</ul>
</div>
</div>
</section>
<h2>What is data ethics?</h2>
<section>
<div>
<h3>What is Data Ethics?</h3>
<blockquote>
<p>&ldquo;Data ethics is a branch of ethics that evaluates data practices with the potential to adversely impact on people and society &ndash; in data collection, sharing and use&rdquo;</p>
<cite>The Open Data Institute</cite></blockquote>
<p>When we consider the broader philosophy of ethics, we consider what is right and what is wrong. The concept of right and wrong are personal to the individual and are informed by one's life experiences, cultural values and the systems in which they have grown up.</p>
<p>That being said, the practice of data ethics is not to be confused with ethics more broadly. Data ethics is a branch of ethics and considers data practices. In short, in our professional practice we undertake actions, build tools and provide services that have an impact.</p>
<p>By considering data ethics as a professional practice we explore the impact of these outputs and consider the intended consequences and the unintended consequences.</p>
</div>
<div>
<h3>Understanding the impact of products and services</h3>
<p>Research undertaken by Doteveryone (Miller &amp; Coldicutt, 2020) provided evidence that tech workers believe in the power of their products to drive positive change. However, the level of positive change is dependent on their ability to raise concerns, work with broad ranges of stakeholders with different expertise and ultimately understand the possible outcomes of their work - both positive and negative.</p>
<p>A key finding from the research published by Doteveryone found that:</p>
<blockquote>
<p>"More than a quarter (28%) of tech workers in the UK have seen decisions made about a technology that they felt could have negative consequences for people or society. Nearly one in five (18%) of those went on to leave their companies as a result."</p>
</blockquote>
<p>Within this research, <strong>DotEveryone found that 78% of workers want practical tools</strong> to support them in exploring the impact of their products.</p>
</div>
<div>
<h3>The data lifecycle</h3>
<p>Depending on your objective, you will be using data at any point, and possibly multiple points across the data lifecycle. This may include any of the following:</p>
<ul>
<li>Creation, curation or acquisition of data</li>
<li>Storage of data (including retention)</li>
<li>Organising and structuring of data</li>
<li>Algorithmic model building</li>
<li>Algorithmic model deployment</li>
<li>Sharing or selling of data, and finally</li>
<li>The disposal of data.</li>
</ul>
<p>Each stage in the data lifecycle has different ethical considerations that affect people and need to be considered. The process of examining our impact is not always clear, or straightforward.</p>
</div>
<div>
<h3>Data Ethics in Practice (... or not)</h3>
<p>Unfortunately, there is no shortage of examples of unethical data practices. Let&rsquo;s explore some:</p>
<div>
<h4>Auto Cropping images on Twitter</h4>
<p>Back in January of 2018, Twitter introduced an auto-cropping AI that detects the most interesting part of your image and crops the &lsquo;preview&rsquo; photo to match.</p>
<p>Watch the video to learn more about the effects of Twitter&rsquo;s auto cropping algorithm.</p>
<div>
<div><a href="https://player.vimeo.com/video/817021580?h=4f28e6368a&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" target="_blank" rel="noopener">video link</a></div>
</div>
</div>
<div>
<h4>Diagnosing Skin Cancer</h4>
<p>With skin cancer. Like most diseases, early diagnosis is a key contributor to positive health outcomes. In the case of a Melanoma diagnosis, an individual is 100% likely to survive over 1 year if it is diagnosed at the earliest stage. This statistic decreases to 53% when diagnosis occurs at the latest stage. Individuals with non-white skin, whilst less likely to develop skin cancer, have been found to have lower 5-year survival rates (70%) than white individuals (92%).</p>
<p>Watch the video to learn more about the effects of bias in training data sets.</p>
<div>
<div><a href="https://player.vimeo.com/video/817240774?h=1cdc6f2d90&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" target="_blank" rel="noopener">video link</a></div>
</div>
</div>
</div>
<div>
<h3>Why is data ethics important?</h3>
<p>Data has the potential to highlight the bias and assumptions that exist across society. Within products and services, these &lsquo;data-driven&rsquo; biases can sometimes create outcomes that are incorrect, exclusionary or outright harmful.</p>
<p>Understanding the potential for both positive and negative consequences is integral to the delivery of effective and valuable products and services. However, we must also recognise that important skills of the future are not only technical, but rely on the skills of a broad range of stakeholders that can translate data into information and ultimately into knowledge.</p>
<p>The value of data-driven decision making may not always be the panacea it&rsquo;s intended to be. Understanding the implications of bias at any point during the data lifecycle, whether that is at the point of collection, analysis, model development and deployment, as well as at the point of disposal is imperative to ensuring a human-centred outcome.</p>
<p>Exploring the data ethics in and around the vast technological developments is vital to ensuring that contributions to the future of AI and tech-enabled solutions are both effective and equitable.</p>
<div>
<div><a href="https://player.vimeo.com/video/817017824?h=4a16c39eed&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" target="_blank" rel="noopener">video link</a></div>
</div>
</div>
</section>
<h2>The Consequence Scanning Toolkit</h2>
<div>
<div>
<h4>There is never a lack of data, or technology</h4>
<p>There is never a lack of technology or data. Both are often seen as the &ldquo;golden bullet&rdquo; to solving a whole host of vaguely defined problems. In fact having too much technology and data can cause more problems, both legal and ethical. Too much data and technology becomes more challenging to manage, more disparate, siloed and not to mention aged and part of the legacy/debt of the organisation. Having the right processes and people in place to support the data and technologies and being focussed on your purpose will help ensure that technologies and data are suitable for the purpose and not there because it is easy to get both.</p>
</div>
<div>
<h3>The Consequence Scanning Toolkit</h3>
<p>Consequence Scanning is an agile practice for responsible innovators. It's a framework of practice to enable teams who work on data enabled projects to explore the impact of their practice by identifying the intended and unintended consequences, and defining the appropriate actions to mitigate negative effects and promote positive impact.</p>
<p>In this section, we will explore the consequence scanning toolkit and how it can be used in the practice of data ethics.</p>
</div>
<div>
<h3>How to use the Consequence Scanning Toolkit</h3>
<p>You start by asking yourself three questions:</p>
<p>These questions help your team to share knowledge and expertise in order to map the potential impact of your work. Most importantly, these questions are designed to be asked to a varied group of stakeholders ensuring that everyone has the opportunity to input into the project.</p>
<ul>
<li><strong>Intended consequences describe your purpose: </strong>What are you trying to achieve? What is the positive goal of the project? Who will be positively impacted?&nbsp;</li>
<li><strong>Unintended consequences describe the potential side-effects </strong>of your project and focus on who could be negatively impacted or excluded from your project, impact on wider society, economy or even environment.</li>
</ul>
</div>
<div>
<h4>When should you do Consequence Scanning?</h4>
<p>You should carry out consequence scanning during the early stages of product planning and return to your thinking and outputs throughout development and maintenance.</p>
</div>
<div>
<h3>Striking a balance between positive and negative effects</h3>
<p>To assess if a potential unintended consequence is a real risk it is important to evaluate the likelihood and the severity of the impact. A common method of doing this is to complete a risk matrix and assign a risk score.</p>
<p>It is important to remember that while the impact of different risks could be the same, e.g. &ldquo;Damage to organisations reputation&rdquo;, the likelihood and severity scores could be significantly different depending on the risk and wider context of the risk within the organisation. It is also likely to vary widely depending on the organisation's attitude towards the risk. For example, is sharing poor quality data, which is likely most of the time, actually high in severity? Especially if you tell people the data is likely to contain errors due to the way it was collected.</p>
</div>
<div>
<h3>The recommended structure</h3>
<p>If you're working in a group and looking to run your own Consequence Scanning workshop, the following structure will support you in ensuring each individual has the opportunity to construct their own ideas as well as collaborate with others.&nbsp;</p>
<p>Phase one:</p>
<ol>
<li>Introduction: explain the focus of the event and post up intended consequences.</li>
<li>Quiet time: ask participants the first question. Everyone takes a moment to answer based on their experiences and expertise.</li>
<li>Affinity sorting: participants write down ideas, similar ideas are grouped.</li>
<li><strong>Quiet time: </strong>participants have another opportunity to add new consequences based on other posts.</li>
</ol>
<p>Phase two:</p>
<ol>
<li><strong>Action sorting: </strong>sort the consequences into action categories (act, influence and monitor).</li>
<li><strong>Dot voting: </strong>Once sorted, ask questions 2 (positive consequences to focus on) and 3 (what are the consequences we want to mitigate). Everyone can vote all at once on the consequences.</li>
<li><strong>Discussion: </strong>What do the consequences you can act on mean for your product? Assign responsibilities.&nbsp;</li>
<li><strong>End: </strong>Start working on implementing the actions.</li>
</ol>
</div>
<div>
<h4>An example: the unintended consequences of publishing crime data</h4>
<p>Let&rsquo;s explore a simple example. What do you think the likelihood and severity is related to the following situation?</p>
<ul>
<li><strong>Action: </strong>Releasing crime data with detail at street level</li>
<li><strong>Intended positive impact: </strong>Provide a rich dataset for use in consumer applications, such as travel planning apps that can now find out where high crime areas are and recommend people avoid them, making society safer.</li>
<li><strong>Potential unintended consequence: </strong>A significant reduction in house prices in high crime areas</li>
<li><strong>Perceived negative impact: </strong>People in high crime areas being unable to sell or rent out their houses</li>
</ul>
<p>What do you think the likelihood of the unintended consequence is?</p>
View answer
<p>Many countries have published highly detailed crime data. Although many had concerns, there has been no significant impact as a result of the data being published despite the belief that the release of crime data would affect house prices.</p>
<p>In reality, people simply didn&rsquo;t know what high crime was and it is more about experiences than statistics. There are different things that impact house pricing, and what this perception failed to do was consider the other reasons that drive people to buy houses.&nbsp;</p>
<p>The perceived impact was high, but in actual fact the impact was low.&nbsp;</p>
</div>
<div>
<h3>Deciding your actions</h3>
<p>Once your team has had the opportunity to explore the intended and unintended consequences and rated those highest in risk. You can then group these consequences into the following categories:</p>
<ul>
<li><strong>Act</strong>: The things the core team can take forward and action immediately. Within these, prioritisation and time scales should be applied.</li>
<li><strong>Influence</strong>: Consequences that need to be handled by other stakeholders, either within the organisation or externally, but within reachable networks.</li>
<li><strong>Monitor</strong>: These are consequences that you cannot control, such as political climate or global trends.</li>
</ul>
<p>From this point, you can articulate the required roles and responsibilities, allocate the actions and start implementing them against your projects.</p>
</div>
<div>
<h3>Applying the Consequence Scanning Toolkit</h3>
<p>The Consequence Scanning toolkit is designed to be adaptable and fit into how you already work with your team. It has been designed to be:</p>
<ol>
<li><strong>Lightweight</strong>: minimal materials, preparation or technical expertise required.</li>
<li><strong>Simple to start:</strong> easy to understand what you should do, how and why.</li>
<li><strong>Difficult to master:</strong> critical and creative thinking is required. Participants must think openly and speak candidly about the potential harms.</li>
</ol>
<p>Although it may be difficult at first, consequence scanning becomes easier the more you embed it into your practices. In the next section we'll introduce some real-world data ethics examples and provide an opportunity for you to consider the consequences, both intended and unintended as well as positive and negative.&nbsp;</p>
</div>
</div>
<h2>Applying data ethics to real-world scenarios</h2>
<div>
<div>
<h4>Identifying consequences</h4>
<p>In this section, we present data ethics case studies from the real world. For each case study, you should focus on the first phase of the Consequence Scanning Toolkit and try to:</p>
<ol>
<li>identify the intended and unintended consequences,</li>
<li>decide on which positive consequences to focus on, and</li>
<li>decide which consequences need to be mitigated.</li>
</ol>
<p>To support your learning each case study is accompanied with sample outcomes.</p>
</div>
<div>
<h3>Should we close fire stations?</h3>
<p>Throughout this course we have been looking at real policy decisions made to close 10 fire stations in London. Given the sensitivity of such a topic it is an ideal candidate for evaluating the ethical challenges and risks of taking such action.</p>
</div>
<div>
<h3>Closing fire stations: Consequence Scanning</h3>
<p>Ask yourself the following:</p>
<ul>
<li>What are the intended consequences and benefits?</li>
<li>What are the potential obstacles that might be faced?</li>
<li>What do we know about the current situation from the data?</li>
<li>Can any inferential statistics be used to model the potential consequences?</li>
<li>How can we present our findings clearly to audiences to paint a clear picture of the benefits and how we will address the risks?</li>
</ul>
View Possible Answers
<p>Closing the fire stations will provoke an emotive and negative response. However, looking at the data we have seen that performance is not fair across London, in fact it is worse where people live. Presenting the data in clear and concise ways will help people understand the situation. More than this however, being open and transparent with the data, analysis, expected positive and negative outcomes helps even more.</p>
</div>
<div>
<h3>How expensive is it to play Pokemon Go?</h3>
<p>When Pokemon Go was released, users noted that pokestops and gyms occurred much less frequently in poor neighbourhoods, and black and other minority neighbourhoods. This was due to bias in the dataset used to create the locations.</p>
<h5>What is Pokemon Go?</h5>
<p>Pokemon (short for the original Japanese title of &lsquo;Pocket Monsters&rsquo;) is a popular franchise owned by Nintendo with two other companies (Game Freak and Creatures). Pokemon are imaginary creatures and players (known as trainers) can catch these animals and use them to battle other trainers.</p>
<p>Pokemon Go is a free-to-play, location-based augmented reality game developed by Niantic in collaboration with Nintendo. In the game, players use their mobile device GPS to locate, capture, battle and train virtual Pokemon. Players collect &lsquo;pokeballs&rsquo; &ndash; needed to catch pokemon, and therefore to play the game &ndash; from pokestops (check points) and battle other pokemon at &lsquo;gyms&rsquo;, which are based on their GPS location. Without access to pokestops, players must pay for pokeballs and other items. Players with easy access to pokestops and gyms, on the other hand, can earn more experience points and collect more items for free.</p>
<h5>Where did the data come from?</h5>
<p>The locations of pokestops and gyms in Pokemon Go were derived by Niantic from the locations of &lsquo;portals&rsquo; in a previous augmented-reality GPS-based game, Ingress.</p>
<h5>Who collected the data?</h5>
Locations for Ingress were predominantly crowd-sourced, both via an existing database &ndash; Historical Marker Database &ndash; and by Ingress players. The Historical Marker Database reflected contributions from approximately 3,000 volunteers across the US who were mostly male. Ingress players also appeared to be mainly male, young and English-speaking.</div>
<div>
<h3>Pokemon Go: Consequence Scanning</h3>
<p>Ask yourself the following:</p>
<ul>
<li>What are the intended consequences of Pokemon Go?</li>
<li>What are the unintended consequences of Pokemon Go?</li>
<li>Which intended consequences should Pokemon Go look to maximise?</li>
<li>Which unintended consequences should Pokemon Go mitigate?</li>
<li>Which of the consequences are within Pokemon Go's capability to manage?</li>
</ul>
View possible answers
<p>Studying the locations of Pokestops, derived from Ingress data, exposes the limitations of crowd-sourced data. In Pokemon Go &ndash; which has had significantly broader reach than Ingress &ndash; gaps in the underlying pokestop location data stand to potentially drive up the cost of playing Pokemon Go for users in predominantly black, hispanic or low socio-economic neighbourhoods.&nbsp;</p>
<p>The underlying data sources for the locations of pokestops in Pokemon Go are non-personal, however these geolocation gaps stand to negatively impact on certain Pokemon Go users (by increasing the overall cost of participation). A data ethics framework that helps people to identify how data use affects different demographics could have helped identify this issue before the launch of the game and allowed the developer to take steps to mitigate the issue and broaden the pool of players.</p>
<p>Of course, these are only some of the possible consequences. You may have identified others.</p>
</div>
<div>
<h3>The demand for Uber</h3>
<p>Uber uses data about their users to provide them with personalised pricing and create a more efficient transport market where supply matches demand. Although no personal data was exposed, Uber&rsquo;s model nevertheless put people at risk in an emergency situation. In 2014, a hostage situation in Sydney led to people feeling fearful for their lives and wanting to escape the area by any means possible. Many didn&rsquo;t feel safe to do so via public transport, thus tried to book an Uber, only to find that prices had surged to four times higher than usual, a level never seen by many.&nbsp;</p>
<p>Uber issued a statement on Twitter saying: &lsquo;We are all concerned with events in CBD. Fares have increased to encourage more drivers to come online &amp; pick up passengers in the area&rsquo;. However, the fare-calculating algorithm raised those fare prices by up to four times the normal rate. This meant the minimum fare per trip was $100.</p>
<p>This led to an outcry on social media causing Uber to respond by offering free rides and refunds for people charged with higher rates.&nbsp;</p>
<p>Uber faced a huge backlash. Even if the surge model is designed to encourage more drivers onto the road, is it ethically acceptable to (1) Encourage drivers to move into a potentially dangerous area and (2) charge people such high fares to escape the area.&nbsp;</p>
</div>
<div>
<h4>Uber Surge Pricing: Consequence Scanning</h4>
<p>Ask yourself the following:</p>
<ul>
<li>What are the intended consequences of surge pricing?</li>
<li>What are the unintended consequences of surge pricing?</li>
<li>Which intended consequences should Uber look to maximise?</li>
<li>Which unintended consequences should Uber mitigate?</li>
<li>Which of the consequences are within Uber&rsquo;s capability to manage?</li>
</ul>
View possible answers
<p>Following the 2014 crisis, Uber issued an apology: &ldquo;We didn't stop surge pricing immediately. This was the wrong decision.&rdquo; Since then, Uber has defended its surge pricing strategy in other cities, and reached an agreement with regulators in the US to restrict the policy during national emergencies (capped at 2.1x).&nbsp;</p>
<p>In November 2017, Uber responded to requests of drivers to adjust its surge pricing policy in the US and Canada, and implemented a public program (&lsquo;180 Days of Change&rsquo;) to make improvements to the driver experience. This included the following benefits to drivers and customers:&nbsp;</p>
<ul>
<li>Making surge periods last longer to provide more time for drivers to surge areas</li>
<li>New surge pricing will be made based on the location of the driver, not the passenger ie when drivers reach the surge area, an extra dollar amount will be added to their next trip regardless of where the passenger is located</li>
<li>Surges will be simpler to understand eg by telling drivers upfront exactly how much additional they&rsquo;ll be paid in surge fees for the trip</li>
</ul>
More recently, regulators in NYC have introduced measures to regulate the rideshare industry and create more &lsquo;fairness&rsquo; for drivers- such as introducing minimum pay rate for rideshare drivers, and regulating the number of licences/vehicles to prevent saturating the market. However, this could lead to more surge-pricing periods with a capped supply of drivers.</div>
<div>
<h3>Weighing up the wrong risk factors</h3>
<p>Online car insurance firms use predetermined algorithms to assess the risk of a user filing a claim against their policy. At the beginning of 2018, this practice faced public backlash when the Sun newspaper found that insurance quotes for drivers with the traditional English name &lsquo;John&rsquo; were far lower than quotes of the same for drivers named &lsquo;Mohammed&rsquo;.</p>
<h5>The impact</h5>
<p>Under particular scrutiny was the insurance company Admiral where their deal on Go Compare for fully comprehensive insurance on a 2007 Ford Focus in Leicester was priced at &pound;1,333 for &lsquo;John Smith&rsquo; and &pound;2,252 for &lsquo;Mohammed Ali&rsquo; - a price difference of &pound;919. Sixty quotes run across ten different cities on GoCompare and other comparison sites revealed the companies in question consistently charged more if the driver was called Mohammed.</p>
<h5>Sourcing the data</h5>
<p>These algorithms are based on key indicators about users which are measured against a data set to weigh risk and calculate premium rates. Common rating factors vary but include age, demographic, marital status and driver history i.e. if you have had accidents in the past, an algorithm will assume that you are likely to have more accidents in the future.</p>
<p>In this case, the training set for the model contained all of these factors but also included the drivers&rsquo; names which should not be relevant. As a result the model probably ended up selecting and assigning significant weight to names that occurred frequently in the data, whether as a simple one-to-one mapping (very common name = more incidents) or as a proxy for a combination of other, more relevant attributes like age, gender and location (young males in urban areas = higher risk).</p>
</div>
<div>
<h3>Car insurance: Consequence Scanning</h3>
<p>Ask yourself the following:</p>
<ul>
<li>What are the intended consequences of insurance risk models?</li>
<li>What are the unintended consequences of insurance risk models?</li>
<li>Which intended consequences should insurance providers look to maximise?</li>
<li>Which unintended consequences should insurance providers mitigate?</li>
<li>Which of the consequences are within insurance providers' capability to manage?</li>
</ul>
View Possible Answers
<p>The algorithms are based on key indicators about users which are measured against a data set to weigh risk and calculate premium rates. Common rating factors vary but include age, demographic, marital status and driver history i.e. if you have had accidents in the past, an algorithm will assume that you are likely to have more accidents in the future.</p>
<p>In this case, the training set for the model contained all of these factors but also included the drivers&rsquo; names which should not be relevant. As a result the model probably ended up selecting and assigning significant weight to names that occurred frequently in the data, whether as a simple one-to-one mapping (very common name = more incidents) or as a proxy for a combination of other, more relevant attributes like age, gender and location (young males in urban areas = higher risk).</p>
<p>Following public outcry, the Financial Conduct Authority (FCA) announced it would extend its <a href="https://ethicsandinsurance.info/2018/04/10/dual-pricing-in-insurance/" target="_blank" rel="noopener">consumer data research</a> to examine whether names can impact the cost of car insurance cover. This is part of a widespread investigation into dual pricing and &lsquo;optimal pricing&rsquo; tactics within the insurance industry during 2018/19.</p>
<p>In future, insurers will need to better understand these issues to prepare a response that the regulator can judge to be informed and reasonable. Insurers need to be clear on the principles that underpin their pricing strategy, and have a clear template for how those principles are being applied within all those pricing algorithms in widespread use e.g. what signals are pricing algorithms looking for in the data.</p>
</div>
<div>
<h3>Considerations for data ethics</h3>
<p><strong>A data project can involve only non-personal data and still be unethical</strong></p>
<p>Issues of bias, inaccuracies and inconsistencies in data can arise regardless of the nature of its source, whether personal or non-personal. Researchers analysing trends in aggregated search-engine data, for example, must account for gaps in who has contributed to that data (a predominantly digitally literate population).&nbsp;</p>
<p>Identifying and mitigating issues within data sources that could negatively affect certain population demographics if left untreated is part of treating data ethically.</p>
</div>
</div>
<h2>Reflecting on your practice</h2>
<section>
<div>
<h3>Reflecting on your practice</h3>
<p>To support your learning we recommend taking the opportunity to reflect on the module and examine how the lessons covered can be applied to your own professional practice.</p>
<p><strong>To aid the process, we've provided a <a href="@@PLUGINFILE@@/SDS%20Module%205%20Reflective%20Template.docx" target="_blank" rel="noopener">SDS Module 5 Reflective Template (.docx)</a>.&nbsp;</strong></p>
<p>The purpose of this document is to give you an opportunity to explore how your learning can apply to your professional practice. It is not an assessed piece of work. Instead, it is intended to provide a discussion framework for you and your line manager, mentor or colleagues.&nbsp;</p>
<p>We recommend that the reflections you make during this course are used as part of a portfolio of learning and development, used in conjunction with your own professional development frameworks.</p>
</div>
</section>
<h2>Applying data ethics: Consequence Scanning Toolkit</h2>
<p>(Scorm/Adapt Activity)</p>
<div>
<h4>There is never a lack of data, or technology</h4>
<p>There is never a lack of technology or data. Both are often seen as the &ldquo;golden bullet&rdquo; to solving a whole host of vaguely defined problems. In fact having too much technology and data can cause more problems, both legal and ethical. Too much data and technology becomes more challenging to manage, more disparate, siloed and not to mention aged and part of the legacy/debt of the organisation. Having the right processes and people in place to support the data and technologies and being focussed on your purpose will help ensure that technologies and data are suitable for the purpose and not there because it is easy to get both.</p>
<h5>Consequence Scanning</h5>
<p>Consequence Scanning is an agile practice for responsible innovators. It's a framework of practice to enable teams who work on data enabled projects to explore the impact of their practice by identifying the intended and unintended consequences, and defining the appropriate actions to mitigate negative effects and promote positive impact.&nbsp;</p>
<p>In this section, we will explore the consequence scanning toolkit and how it can be used in the practice of data ethics.&nbsp;</p>
Launch e-learning
<p><em>If you are viewing this page using the Moodle app,&nbsp;<a href="https://visionary-brigadeiros-d131e8.netlify.app/wlt-sds-async-module-6-enabling-positive-impact/build/#/id/64830022b38aaf6b93dd9fdd" target="_blank" rel="noopener">click here</a>&nbsp;to launch the e-learning</em></p>
<div>
<div>CLOSE AND RETURN TO COURSE</div>
<div>Loading...</div>
</div>
</div>
<h2>Next steps in data ethics</h2>
<section>
<div>
<h3>Enabling positive impact: next steps in data ethics</h3>
<p>For some time, the Open Data Institute has been working on measures to help organisations build trust in how they collect, use and share data, and to foster better use of data overall. Trust is an essential component of society. When trust breaks down, the public lose faith in the institutions that provide them with services, and organisations lose the ability to share data and collaborate in ways that could improve all our lives.</p>
<p>By exploring data ethics in depth, the ODI uncovered common assumptions about data ethics that we wanted to challenge.</p>
<ul>
<li>&lsquo;Data ethics is only an issue where activities involve personal data.&rsquo; Ethics issues must also be considered in the collection and use of non-personal data. For example, not publishing the location of bus stops in poorer neighbourhoods can mean that the advantages of smartphone-mapping and route-finding services are not available to people who live in those areas, increasing existing inequalities.</li>
<li>&lsquo;Data ethics is concerned with data protection compliance.&rsquo; Complying with legal obligations &ndash; under the EU General Data Protection Regulation, for example &ndash; is just one part of treating data ethically. Data-related activity can be unethical but still lawful.</li>
<li>&lsquo;Data ethics is (only) about how organisations use data.&rsquo;&nbsp;Data ethics is about the impact that all data activities have on people and society. Collecting and sharing data only about certain groups of people may disadvantage them relative to others. All activities should be subject to ethical examination.</li>
</ul>
</div>
<div>
<h3>Data ethics tools</h3>
<p>The ODI has developed and is responsible for a series of data ethics tools that focus on supporting individuals and organisations in embedding data ethics practices into their projects and overarching organisational strategies.</p>
Project Level
<h3><a href="https://doteveryone.org.uk/project/consequence-scanning/" target="_blank" rel="noopener">Consequence Scanning Toolkit</a></h3>
<p>For a quick assessment of a project, with particular emphasis on the intended and unintended consequences on people, communities and the planet. It provides an opportunity to enhance positive consequences, and to mitigate or address potential harms from negative consequences before they happen. It considers what can be acted upon, what can be influenced, and how to monitor.</p>
<h3><a href="https://www.theodi.org/article/the-data-ethics-canvas-2021/" target="_blank" rel="noopener">Data Ethics Canvas</a></h3>
<p>For deep consideration of a project. It is divided into four areas: &lsquo;Data&rsquo;, &lsquo;Impact&rsquo;, &lsquo;Engagement&rsquo; and &lsquo;Process&rsquo;. The Data Ethics Canvas is a tool for anyone who collects, shares or uses data. It helps identify and manage ethical issues &ndash; at the start of a project that uses data, and throughout.</p>
<p>It encourages you to ask important questions about projects that use data, and reflect on the responses. These might be:</p>
<ul>
<li>What is your primary purpose for using data in this project?</li>
<li>Who could be negatively affected by this project?</li>
</ul>
<p>The Data Ethics Canvas provides a framework to develop ethical guidance that suits any context, whatever the project&rsquo;s size or scope.</p>
Organisational Level
<h3><a href="https://www.theodi.org/article/data-ethics-maturity-model-benchmarking-your-approach-to-data-ethics/" target="_blank" rel="noopener">Data Ethics Maturity Model</a></h3>
<p>Unlike the other tools, the Data Ethics Maturity Model is not intended for specific data sets. Considering data ethics at project level is good, but deep and lasting developments must happen at organisation level. This is the focus of the Data Ethics Maturity Model. The tool helps assess and benchmark how widely embedded data ethics culture and practices are across your organisation.</p>
<p>It provides a framework to consider current performance and future ambitions, to embed a culture of ethical data practices. This tool builds on the Open Data Institute&rsquo;s <a href="https://www.google.com/url?q=https://www.theodi.org/article/the-data-ethics-canvas-2021/&amp;sa=D&amp;source=editors&amp;ust=1686323699302615&amp;usg=AOvVaw1XyxRe2FklFsMHRqvZOV1j" target="_blank" rel="noopener">Data Ethics Canvas</a>, which helps identify and manage ethical issues within a specific project, and may be particularly helpful to <a href="https://www.google.com/url?q=https://www.theodi.org/event_series/data-ethics-professional/&amp;sa=D&amp;source=editors&amp;ust=1686323699303019&amp;usg=AOvVaw2bXydGcgIFNrholHr1ezNL" target="_blank" rel="noopener">Data Ethics Professionals</a>&nbsp;who are working to embed a data ethics culture within their organisation.</p>
</div>
<div>
<h4>&nbsp;Learn more with the ODI</h4>
<div>
<p>We offer a mix of short introductory courses, and longer courses that offer opportunities to really explore the area of data ethics and even become certified by the ODI as a data ethics professional.&nbsp;</p>
Data Ethics Essentials
<p>The aim of the &lsquo;Data Ethics Essentials in AI&rsquo; course is to emphasise the significance of data ethics and enable learners to develop their understanding of data ethics as a domain, examine how data ethics manifests itself in data-enabled products and services, such as machine learning and AI, and empower learners to examine their own direct impact and incorporate ethical considerations into their own professional practice.&nbsp;</p>
<p>The course is for individuals who use data in their professional roles, but are new to data ethics. The sequential modules introduce the concept of data ethics and its importance in today&rsquo;s society with increasing influences from new technologies, such as generative AI. The course then explores the available data ethics tools and frameworks, providing a basis for learners to apply these tools in their own professional practice.</p>
<h5>Educational aims</h5>
<p><strong>The course aims to develop greater skills and knowledge in:</strong></p>
<ul>
<li>Understanding the importance of data ethics as a domain</li>
<li>Explore data ethics in the context of AI and other data related projects</li>
<li>Examining data projects to identify the intended and unintended consequences</li>
<li>Evaluate the ethical challenges associated with data, and develop awareness of potential biases that can arise through the use of data-enabled decision making&nbsp;</li>
<li>Identifying common barriers to adopting approaches to data ethics</li>
<li>Applying practical tools to investigate data projects through the lens of data ethics&nbsp;</li>
<li>Developing an actionable approach to managing data ethics consequences</li>
</ul>
<a href="https://learning.theodi.org/courses/introduction-to-data-ethics" target="_blank" rel="noopener">Learn more about our data ethics learning offering on our website</a>.Data Ethics Professionals
<p>Unlock the potential of responsible data creation, sharing and use, and become a sought-after data ethics professional with our comprehensive course designed for forward-thinking individuals passionate about evaluating data practices with the potential to adversely impact on people and society. Learn to build trust, be transparent, and accountable in the data-driven world.</p>
<p>This course is best suited to professionals who manage or use data to inform business strategy as well as day to day decision making. This includes, but is not limited to, performing data organisation activities with raw data, performing analysis either in spreadsheet software or BI Tools as well as interpreting data outputs to share with stakeholders.</p>
<p>Successful participants will be:</p>
<ul>
<li>Recognised as competent in the area of data ethics and the practical steps required to help organisations minimise potential harms that come from data collection, use and sharing.</li>
<li>Able to think critically about data ethics.</li>
<li>Ready to apply tools and techniques to analyse the current situation within organisations.</li>
<li>Equipped to suggest potential actions to minimise risk &ndash; be it through process adoption as guided by tools such as the data ethics canvas, or changes to the way the data itself is collected, used, or shared.</li>
</ul>
<h5>Educational aims</h5>
<p><strong>The course aims to develop greater skills and knowledge in:</strong></p>
<ul>
<li>A deep understanding of data ethics as a domain</li>
<li>Practical expertise in data and techniques to evaluate ethical and legal risks</li>
<li>Ability to perform a professional piece of research, drawing on historical and peer-reviewed literature</li>
<li>Ability to identify and use a number of tools to help evaluate ethical impacts and suggest actions to minimise harmful impacts from data</li>
<li><a href="https://learning.theodi.org/courses/data-ethics-professional" target="_blank" rel="noopener">Learn more about the course on our website</a>.</li>
</ul>
</div>
</div>
</section>
<h2>Module overview</h2>
<section>
<div>
<h3>Module 5 overview</h3>
<p>We present data to our stakeholders by telling stories. To accompany the story, we produce visualisations that support the explanation of the scale and impact of our data.</p>
</div>
<div>
<h3>Learning outcomes</h3>
<p>By the end of this section you will understand the purpose of APIs, and how to work with live data.</p>
<p><strong>To achieve this, you will:</strong></p>
<ul>
<li>Explain the characteristics of big data</li>
<li>Evaluate the impact of these characteristics when working with live data</li>
<li>Explain the differences of working with live and non-live data</li>
<li>Work with live data</li>
</ul>
</div>
</section>
<h2>Video: Data futures</h2>
<section>
<div>
<h4>Video</h4>
<p>In this video, Dr David Tarrant looks at the emergence of Big Data and how the techniques looked at in this course are more relevant than ever to roles that involve making decisions from data.&nbsp;</p>
<a href="https://player.vimeo.com/video/292301015" target="_blank" rel="noopener">video link</a>
<p>The next part of the course looks in more detail at the concepts covered in this video and looks at the role of APIs in both making data available and using data effectively.</p>
</div>
</section>
<h2>Working with big data</h2>
<p>(Scorm/Adapt Activity)</p>
<div>
<h4>Working with big data</h4>
<p>Big data is a frequently used term that is associated with advances in technologies and science that can both store and process ever diverse and large quantities of data.</p>
Launch e-learning
<p>This module looks at what big data is and what to be aware of when working with it.</p>
<p><em>If you are viewing this page using the Moodle app,&nbsp;<a href="https://visionary-brigadeiros-d131e8.netlify.app/wlt-sds-async-module-5-open-data-futures/build/#/id/647dedd8b38aaf6b93dd9f0c" target="_blank" rel="noopener">click here</a>&nbsp;to launch the e-learning</em></p>
<div>
<div>CLOSE AND RETURN TO COURSE</div>
<div>Loading...</div>
</div>
</div>
<h2>Having a REST with API design</h2>
<p>(Scorm/Adapt Activity)</p>
<div>
<h4>Having a REST with API design</h4>
<p>Consistent availability of data is key to enabling reuse. This module looks at how to design and implement an Application Programming Interface (API) for data. More specifically, this module looks at the importance of an API for resources and the relation of this to linked open data.</p>
Launch e-learning
<p><em>If you are viewing this page using the Moodle app,&nbsp;<a href="https://visionary-brigadeiros-d131e8.netlify.app/wlt-sds-async-module-5-open-data-futures/build/#/id/647dedd7b38aaf6b93dd9f0a" target="_blank" rel="noopener">click here</a>&nbsp;to launch the e-learning</em></p>
<div>
<div>CLOSE AND RETURN TO COURSE</div>
<div>Loading...</div>
</div>
</div>
<h2>Hands-on with APIs</h2>
<p>In this exercise, you are going to use a couple of different types of Application Programming Interfaces (API) to access data. They are both web based APIs but work in slightly different ways.</p>
<p>The first API we are going to look at is a REST API.</p>
<h3>REST APIs</h3>
<p>REpresentational State Transfer (REST) or RESTful web services allow systems to interact with resources over the web.</p>
<p>Resources were first defined on the web as documents or files identified by their URLs - the paths where they can be found. Today however it is more useful to think of these URLs as Identifiers (URIs). Thus a URI can have an abstract definition and refer to ANY entity that can be identified, named, addressed or handled, in any way whatsoever on the web!</p>
<p>In the first part of this exercise we are going to look at how we can create an API for accessing data about UK Railway Stations.</p>
<p>This data comes from the&nbsp;<a href="https://naptan.app.dft.gov.uk/datarequest/help" target="_blank" rel="noopener">NAPTAN dataset</a>&nbsp;and a subset of this (just the Railway References)&nbsp;has been made available for this exercise via an example API endpoint. If you want to browse the whole dataset containing every transport access point in the UK then a 136Mb (at the time of writing) CSV file is available from the <a href="https://naptan.app.dft.gov.uk/datarequest/help" target="_blank" rel="noopener">Department for Transport</a>.&nbsp;</p>
<p>So for example you can get data about Penzance:</p>
<p><a href="http://api-demo.learndata.info/NAPTAN/CrsCode/PNZ" target="_blank" rel="noopener">http://api-demo.learndata.info/NAPTAN/CrsCode/PNZ</a></p>
<p>If you click on this link it will show you a web page containing the data. Note also that the URL has also changed to include a .html on the end. This happened for two reasons:</p>
<p><strong>1</strong>. The resource above is an identified we have created for Penzance station. The station itself, not the web page that describes the station. This means that we can interact with the resource itself, asking for different versions in this case.</p>
<p>[<strong>Note</strong>] Many APIs are created to reference real world resources that we can interact with. This is often referred to as the internet of things. So if you have an internet connected thermostat then it might have an API identifier and you could send a number to this URI in order to set the temperature you want in your house. So you are interacting with the physical thing being identified just like pressing a button on the front of it</p>
<p><strong>2</strong>. Given (1), the second reason we ended up at a new location on the web is to do with the interaction that we just made by clicking on the resource identifier. By clicking on the link, your web browser requested a web page about the thing. The server simply responded by telling the browser where that version of this resource is, in this case at a location that has the .html on the end.</p>
<p>Most web pages you visit on the web don&rsquo;t show you the .html filetype on the URL. This is because machines use a function called&nbsp;<strong>content negotiation</strong>&nbsp;to state the format of interaction they are requesting, in the case of a web browser it will always request a text/html representation of the resource.</p>
<p>text/html is one of many mime-types which are used to define a file format. There are thousands of registered mime types but in the world of data we are interested in just a few key ones:</p>
<ul>
<li>text/csv - .csv &ndash; Comma Separated Values</li>
<li>application/json - .json &ndash; JavaScript Object Notation</li>
<li>application/xml &ndash; eXtensible Markup Language*</li>
</ul>
<p><em>* not available in this exercise</em></p>
<p>For examples on how content negotiation works in machine language see the next lesson.</p>
<p>As content negotiation is hidden from a human by the machine it is not that easy to directly use without writing code. However as you will discover in this exercise, content negotiation is the much better way to interact with an API as there is only one URI to reference:</p>
<p><a href="http://api-demo.learndata.info/NAPTAN/CrsCode/PNZ" target="_blank" rel="noopener">http://api-demo.learndata.info/NAPTAN/CrsCode/PNZ</a></p>
<p>This said we will use a human readable method on our API endpoint to keep things clear. The API endpoint in this exercise supports both content negotiation and&nbsp;<em>.file_extensions</em>. So to request different formats for the Penzance station example you can simple add file extensions:</p>
<p><em>HTML (web page) -&nbsp;<a href="http://api-demo.learndata.info/NAPTAN/CrsCode/PNZ.html" target="_blank" rel="noopener">http://api-demo.learndata.info/NAPTAN/CrsCode/PNZ.html</a></em></p>
<p><em>CSV data -&nbsp;<a href="http://api-demo.learndata.info/NAPTAN/CrsCode/PNZ.csv" target="_blank" rel="noopener">http://api-demo.learndata.info/NAPTAN/CrsCode/PNZ.csv</a></em></p>
<p><em>JSON data -&nbsp;<a href="http://api-demo.learndata.info/NAPTAN/CrsCode/PNZ.json" target="_blank" rel="noopener">http://api-demo.learndata.info/NAPTAN/CrsCode/PNZ.json</a></em></p>
<p>So that&rsquo;s it, a simple API for some of the NAPTAN data.</p>
<h5>Try fetching data about different station codes?</h5>
<p>If you don&rsquo;t know them simply refer to the whole list of them you go back from the first request.</p>
<p>That is pretty much all there is to fetching resources using a REST API. For more on managing and updating resources, see the next page.</p>
<h3>Query APIs</h3>
<p>So far we have looked at a REST API for resources that we can identify with web based identifiers. This is very useful for fetching data about particular things that can be identified.</p>
<p>However what if we want to search all the resources and find ones that match certain conditions. In this case you might also need a query API.</p>
<p>A query API allows query parameters to be appended to the end of the API URL to customise the search and return lists of results.</p>
<p>In the case of the NAPTAN data our query API is available from the top level URL:</p>
<p><a href="http://api-demo.learndata.info/NAPTAN/" target="_blank" rel="noopener">http://api-demo.learndata.info/NAPTAN/</a></p>
<p>Clicking on this link will return all stations as you have not yet done a query to filter the results.</p>
<p>The query below shows how you can filter the results. In this case only the results returned will be for stations where the data is on revision number 5.</p>
<p><a href="http://api-demo.learndata.info/NAPTAN/?RevisionNumber=5" target="_blank" rel="noopener">http://api-demo.learndata.info/NAPTAN/?RevisionNumber=5</a></p>
<p>In more advanced APIs you might wish to search for a RevisionNumber greater than 5 however this is not supported by REST so needs specific support in the API.</p>
<p>As mentioned we can combine search queries:</p>
<p><a href="http://api-demo.learndata.info/NAPTAN/?RevisionNumber=5&amp;ModificationDateTime=2007-02-13T17:06:21" target="_blank" rel="noopener">http://api-demo.learndata.info/NAPTAN/?RevisionNumber=5&amp;ModificationDateTime=2007-02-13T17:06:21</a></p>
<p>To make a query the first parameter is preceded by a &ldquo;?&rdquo; and each additional parameter by a &ldquo;&amp;&rdquo;.</p>
<p>Try creating a new set of queries. They work by simply specifying any combination of column_title=value pairs in the query string.</p>
<p>Note that APIs are <strong>very</strong> fussy about spelling and case sensitivity!</p>
<h5>Did you notice that the results are linked data?</h5>
<p>In your results you might have noticed that, unlike in the original CSV data, the results that you get back from the query service contain web based identifiers in the CrsCodeURI column (on the far right).</p>
<p>This means that if you want to find out more information about the resources all you do is &ldquo;click&rdquo; on the link. This is why content negotiation is key, as a machine can do exactly the same thing and interact with the referenced resources and retrieve data.</p>
<p>This is an example of building a web of linked data which both a human and a machine can navigate.</p>
<h5>Why is this important for a data scientist?</h5>
<p>We are just starting to realise the value of a web of linked data. Like the web has both exposed us to an overwhelming amount of information, it has also helped us manage it. The same is true for data. As the volume and variety of data increases, so we are going to need to look to the web for solutions to manage and provide reliable access to this deluge of data. REST and query APIs are an essential tool for a data scientist in order to gain access to a new wealth of reliable data.</p>
<h3>Exploring other datasets</h3>
<p>So far on this course we have explored two main datasets both statistical. That is, they contain statistical or transactional data that involves things.</p>
<p>Both of these datasets have also been loaded into the API service and are available at the following locations.</p>
<p><a href="http://api-demo.learndata.info/Tanzania/" target="_blank" rel="noopener">http://api-demo.learndata.info/Tanzania/</a></p>
<p><a href="http://api-demo.learndata.info/LFB/" target="_blank" rel="noopener">http://api-demo.learndata.info/LFB/</a>&nbsp;(Note that this dataset is really large and must be consumed using queries or URL patterns, e.g.&nbsp;<a href="http://api-demo.learndata.info/LFB/?CalYear=2021&amp;DateOfCall=20/03/2021" target="_blank" rel="noopener">http://api-demo.learndata.info/LFB/?CalYear=2021&amp;DateOfCall=20/03/2021</a>)</p>
<p>Both can be queried using both the REST and Query APIs using the following patterns:</p>
<p>REST API: http://api-demo.learndata.info/{dataset}/{column_title}/{value}</p>
<p>QUERY API:&nbsp;http://api-demo.learndata.info/{dataset}/?column_title=value&amp;column_title2=value2</p>
<p>This work "Hands-on with APIs" &copy;&nbsp;<a href="https://theodi.org/" target="_blank" rel="noopener">Open Data Institute</a>. This work has been funded with the support of the Erasmus+ programme of the European Union. Licensed under a&nbsp;<a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank" rel="noopener">Creative Commons Attribution-ShareAlike 4.0 International Licence</a>.</p>
<p>In the next lesson we recap API design and give some examples of how content negotiation works. Later there is also an extension page that describes how we built api-demo.learndata.info for use on this course.</p>
<h2>Working with live data</h2>
<section>
<div>
<h3>Four stages</h3>
<p>There are four stages to creating value with data. They are:</p>
<p>[Graphic from module 2, reuse] Data, Filter, Visualise, Story.</p>
<p>How we work across these stages changes when we work with live data. Live data refers to real-time information that is continuously updated or refreshed as new data becomes available. It is in contrast to static or fixed data, which remains unchanged unless manually updated.</p>
<p>Live data is often associated with dynamic systems that generate and transmit data in real time, such as sensor networks, financial markets, social media platforms, or weather monitoring stations. There are no limitations on the type of data that can be considered &lsquo;live&rsquo;.</p>
<p>The availability of live data enables applications and services to provide users with the most recent and relevant information. It allows for real-time monitoring, analysis, and decision-making based on the current state of affairs. To access live data, developers often use Application Programming Interfaces (APIs), which provide a structured way to retrieve and interact with the data in real time.</p>
</div>
<div>
<h3>What is an API?</h3>
<p>An API, is a set of rules that allows different applications to communicate with each other. It acts as a bridge between different software systems, enabling them to exchange information.</p>
<p>Imagine you are at a restaurant. The menu you receive is like an API. It provides you with a list of available dishes and their descriptions, and you can choose what you want to order. The kitchen, which prepares the food, is like a software application. It takes your order, processes it, and prepares the dish accordingly. Finally, when your food is ready, it is served to you.</p>
<p>Put simply, an API provides a way for developers to access the functionality and data of a particular software system without needing to understand its internal workings.</p>
</div>
<div>
<h3>How does this change how we work?</h3>
<div>
<table><colgroup> </colgroup>
<thead>
<tr>
<th>Static Data</th>
<th>Live Data</th>
</tr>
</thead>
<tbody>
<tr>
<th>Obtaining Data</th>
<td>Finding data Downloading data Blending data Converting data</td>
<td>&nbsp;</td>
</tr>
<tr>
<th>Scrubbing Data</th>
<td>Filtering Quality checking Validation Cleaning data</td>
<td>&nbsp;</td>
</tr>
<tr>
<th>Analysing Data</th>
<td>Statistical analysis Creating predictions Interpreting findings Validating findings</td>
<td>&nbsp;</td>
</tr>
<tr>
<th>Presenting Data</th>
<td>How to present data Creating visualizations Working with maps Creating dashboards</td>
<td>&nbsp;</td>
</tr>
<tr>
<th>Data Futures</th>
<td>Big data Live data APIs Machine Learning</td>
<td>&nbsp;</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<h2>Module summary</h2>
<section>
<div>
<h4>Data Futures</h4>
<p>[TO BE ADDED]</p>
</div>
</section>